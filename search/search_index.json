{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Race On \u2013 USC Self-Driving Car Competition \u00b6 Challenge Overview \u00b6 The goal of this competition is to engage students in hardware-software development of a self-driving car for racing. Teams of 2 to 4 students compete to build the fastest car and have one semester to prepare for the two races, when all cars, turn by turn, race to record the fastest time. The competition is for USC Viterbi masters students and undergraduate juniors and seniors, regardless of Viterbi program or year. Sophomore undergraduate students are also welcome; however, the difficulty of this competition may be more appropriate for more experienced students. Students are not expected to have prior experience with self-driving cars. Teams will receive instructions how to assemble a car from scratch and program the control algorithm of a self-driving car. All cars are built on the same platform and race against each other on the same track configurations, thus innovation and creativity is what counts. To help the teams get started, a series of workshops on embedded hardware and software concepts will be organized to provide technical support to the students especially during the initial phase of the race. In addition, optional weekly sessions are to be held where teams can work on their projects and interact with other teams. Though these workshops are meant to help the teams develop their car, outside work on the project will be necessary, since an exceptional car will require thoughtful optimization to excel. With these workshops as a starting place, teams are given the unique opportunity to apply their technical knowledge and creative abilities to a real-world situation. Fees and Prizes \u00b6 For new teams, the fee to participate in Race On is $250 per team . This fee covers all the materials needed to assemble a self-driving car, such as the car kit, raspberry pi plus other components and accessories. Teams can get $100 back along with additional sensors and components by actively participating in races. Winning teams even stand a chance to win up to $400 . See Rules section for details. Car Kit \u00b6 Upon registration, new teams will receive their car, described below. Teams with returning members will use their previous car. The car is built around a 1/10 scale RC touring chassis powered by a brushed motor and a 2\u2013cell LiPo battery. The Raspberry Pi microprocessor through image processing will detect the track from the images supplied by a camera and adjust the steering angle and the motor speed to keep the car on the track. The images below show the assembled car, the camera and the Raspberry Pi board that when assemble form the car to be raced. The chassis, battery and the motor are regulated to ensure that no team gains an advantage through better components, and that creativity and superior programming differentiate the teams. Modifications of the chassis are allowed as long as the changes follow the competition rules. Moreover, more sensors can be added as long as the camera remains the main sensor used for steering the car. Cars are permitted to have a wireless connection with a computer for development and calibration, however, during the final race the car should operate autonomously and no external connection is permitted. All cars will be inspected before the final race and only the ones which comply with the above rules will be allowed to compete.","title":"Home"},{"location":"#race-on-usc-self-driving-car-competition","text":"","title":"Race On \u2013 USC Self-Driving Car Competition"},{"location":"#challenge-overview","text":"The goal of this competition is to engage students in hardware-software development of a self-driving car for racing. Teams of 2 to 4 students compete to build the fastest car and have one semester to prepare for the two races, when all cars, turn by turn, race to record the fastest time. The competition is for USC Viterbi masters students and undergraduate juniors and seniors, regardless of Viterbi program or year. Sophomore undergraduate students are also welcome; however, the difficulty of this competition may be more appropriate for more experienced students. Students are not expected to have prior experience with self-driving cars. Teams will receive instructions how to assemble a car from scratch and program the control algorithm of a self-driving car. All cars are built on the same platform and race against each other on the same track configurations, thus innovation and creativity is what counts. To help the teams get started, a series of workshops on embedded hardware and software concepts will be organized to provide technical support to the students especially during the initial phase of the race. In addition, optional weekly sessions are to be held where teams can work on their projects and interact with other teams. Though these workshops are meant to help the teams develop their car, outside work on the project will be necessary, since an exceptional car will require thoughtful optimization to excel. With these workshops as a starting place, teams are given the unique opportunity to apply their technical knowledge and creative abilities to a real-world situation.","title":"Challenge Overview"},{"location":"#fees-and-prizes","text":"For new teams, the fee to participate in Race On is $250 per team . This fee covers all the materials needed to assemble a self-driving car, such as the car kit, raspberry pi plus other components and accessories. Teams can get $100 back along with additional sensors and components by actively participating in races. Winning teams even stand a chance to win up to $400 . See Rules section for details.","title":"Fees and Prizes"},{"location":"#car-kit","text":"Upon registration, new teams will receive their car, described below. Teams with returning members will use their previous car. The car is built around a 1/10 scale RC touring chassis powered by a brushed motor and a 2\u2013cell LiPo battery. The Raspberry Pi microprocessor through image processing will detect the track from the images supplied by a camera and adjust the steering angle and the motor speed to keep the car on the track. The images below show the assembled car, the camera and the Raspberry Pi board that when assemble form the car to be raced. The chassis, battery and the motor are regulated to ensure that no team gains an advantage through better components, and that creativity and superior programming differentiate the teams. Modifications of the chassis are allowed as long as the changes follow the competition rules. Moreover, more sensors can be added as long as the camera remains the main sensor used for steering the car. Cars are permitted to have a wireless connection with a computer for development and calibration, however, during the final race the car should operate autonomously and no external connection is permitted. All cars will be inspected before the final race and only the ones which comply with the above rules will be allowed to compete.","title":"Car Kit"},{"location":"constitution/","text":"Race On Constitution \u00b6 Amended and enforced on September 4, 2020 Preamble \u00b6 The goal of the Race On organization is to provide the opportunity for students to acquire real-world experience related to their course materials in the context of self-driving cars. Article 1 \u2013 Membership \u00b6 All participants of Race On are considered members of the organization. After one semester of participation, a participant can either choose to continue to participate in the Race On competition or be an office bearing member (also office bearer) and take part in shaping the future of the organization. Participants, non office bearers, work as part of a team to record their fastest possible lap on the track and are eligible to win prizes. Office bearers can also participate in the Race On competition for time and rank but they are barred from competing for prizes. Participants do not get any voting or nomination rights in organization elections ;like office bearers do. A person can hold the title of participant as long as that person is not an office bearer. Article 2 \u2013 Office bearers and the organizing board \u00b6 For the first semester, members can only be participants of the Race On competition. After one semester of participation, members can opt to become office bearers. After the first semester of membership as a participant, members are allowed to apply to hold the office of technical specialist to work on one or more of the project areas proposed for the semester. At the beginning of the semester, technical leads and other office bearers select viable candidates from the pool of applicants for holding the office of technical specialist. This office is designed to be equivalent to an engineering internship. Therefore, the earliest a person may be a technical specialist is in their second semester of membership, following one semester of participation in the Race On competition. After at least one semester of holding the office of technical specialist, members may nominate themselves to be voted into an office within the cabinet. This means members can hold cabinet positions only from their third semester as organization members. The number of terms in office for a Technical Specialist is unrestricted. A Technical Specialist works on one or more projects defined by the technical leads and other office bearers. Role functions like an engineering internship with concrete deliverables. The cabinet comprises the president, vice president and technical leads. Members are voted into these offices from a pool of nominees every semester. Additionally, the faculty advisor is considered an executive member of the organization with a permanent seat in the cabinet. All cabinet members get one vote towards the election of technical leads. The President is elected for a term of one semester from a pool of people that held the title of technical specialist or above for at least one semester. The President role is to lead the organization and help its members to achieve the proposed goals. Responsible for the operation of the organization, liaising with USC and departmental administration, and recruiting new members in addition to providing technical assistance where needed. The president is elected among nominated office bearers through general vote by all office bearing members. The president may also cast one extra recorded (non-anonymous, publicly disclosed) vote in any election resulting in a tie. Former presidents have the option to cast one vote towards the selection of projects and technical leads. There may only be one president for the organization at any time. The runner-up in the presidential election is nominated as Vice President. The vice president shares duties and responsibilities with the president including assisting in the leadership of the organization, and recruiting new members. Former vice presidents have the option to cast one vote towards the selection of projects and technical leads. There may only be one vice president for the organization at any time. Technical Leads are elected for one semester term from a pool of people that held the title of technical specialist or above for at least one semester. Technical Leads guids a team of Technical Specialists on a project to add features to or improve the track performance of the car. At the end of every semester, each team will publish an article/blog post as a tutorial and hold workshop sessions as needed with the participants. There may be more than one technical leader for a project. At the same time, a technical leader can lead more than one project. Technical leaders for a project are elected by a majority vote of the president, vice president and past cabinet members after nominations among office bearers. Office bearers may nominate themselves as candidates by proposing project ideas that they want to implement. Candidates will present their projected ideas for the projects they want to lead, which will form the basis for their election to office. A staff or faculty member of USC holds the position of faculty advisor. The faculty advisor oversees the operation of the organization and ensures that the Race On continues to comply with USC policies. There may be one or more faculty advisors for the organization. Article 3 \u2013 Elections \u00b6 Elections must happen for all cabinet offices in the week before the final race. The elections must be ratified by the faculty advisor(s) of the organization and the retiring cabinet members. Section 3a: Eligibility and Nominations \u00b6 Technical specialist candidates must have completed at least one semester as participants before applying for candidacy. (Online form application) All candidates for office bearer posts need to have completed at least one semester as a technical specialist to be eligible for nomination. Technical leads may be nominated on the basis of one or more new project ideas, improvements to old projects, or novel tracks in existing projects. Former and current presidents and vice presidents may nominate themselves as candidates for the same positions only if the entire current cabinet not including themselves unanimously approves of the nomination. Former president and vice president must ensure a smooth power transition and knowledge transfer to the new president and vice president. Section 3b: Voting \u00b6 During a voting cycle, the existing cabinet selects technical specialists from the list of candidates and assigns them to projects based on their preferences. Then, all current office bearers including technical specialists and the cabinet vote for the new president and vice president among the nominated candidates. Now, the newly formed cabinet votes on the new technical leads to be inducted into the cabinet as well as the project ideas that will be pursued for the upcoming semester. Here, former presidents and vice presidents of the organization each get one vote. Article 4: Emergency powers \u00b6 The faculty advisor(s) have full autonomous authority to override all or any part of the constitution including fill all offices including cabinet positions without the need for any approval so long as the goals of the organization as defined in the preamble are not breached. With approval from the faculty advisor(s), the current president and vice president may also fill the entire cabinet without elections in case vacancies of office need to be addressed. Article 5: Projects \u00b6 Current cabinet members and new technical lead candidates can suggest new projects, additional technical tracks based on the proposed projects, or improvements to old projects. The list of projects must be made available on the Race On webpage and updated by the members of the organization. At least one project in each of the following tracks must be fulfilled each semester: Perception, Control, Planning, Simulation It is also crucial that at the end of the semester, the technical lead(s) spearheading the project write a report, technical paper, or blog post detailing their progress. Additionally, workshops and tutorials must be conducted as needed to disseminate the information among all members and participants of Race On. Article 6: Constitutional committees \u00b6 First Constitutional committee (September 4, 2020) comprises Suraj Chakravarthi Raja with Valeriu Balaban, Fernando Valladares Monteiro, Jayson Sia, Amy Puente, Ali Marjaninejad and Thanos Rompokos.","title":"Constitution"},{"location":"constitution/#race-on-constitution","text":"Amended and enforced on September 4, 2020","title":"Race On Constitution"},{"location":"constitution/#preamble","text":"The goal of the Race On organization is to provide the opportunity for students to acquire real-world experience related to their course materials in the context of self-driving cars.","title":"Preamble"},{"location":"constitution/#article-1-membership","text":"All participants of Race On are considered members of the organization. After one semester of participation, a participant can either choose to continue to participate in the Race On competition or be an office bearing member (also office bearer) and take part in shaping the future of the organization. Participants, non office bearers, work as part of a team to record their fastest possible lap on the track and are eligible to win prizes. Office bearers can also participate in the Race On competition for time and rank but they are barred from competing for prizes. Participants do not get any voting or nomination rights in organization elections ;like office bearers do. A person can hold the title of participant as long as that person is not an office bearer.","title":"Article 1 \u2013 Membership"},{"location":"constitution/#article-2-office-bearers-and-the-organizing-board","text":"For the first semester, members can only be participants of the Race On competition. After one semester of participation, members can opt to become office bearers. After the first semester of membership as a participant, members are allowed to apply to hold the office of technical specialist to work on one or more of the project areas proposed for the semester. At the beginning of the semester, technical leads and other office bearers select viable candidates from the pool of applicants for holding the office of technical specialist. This office is designed to be equivalent to an engineering internship. Therefore, the earliest a person may be a technical specialist is in their second semester of membership, following one semester of participation in the Race On competition. After at least one semester of holding the office of technical specialist, members may nominate themselves to be voted into an office within the cabinet. This means members can hold cabinet positions only from their third semester as organization members. The number of terms in office for a Technical Specialist is unrestricted. A Technical Specialist works on one or more projects defined by the technical leads and other office bearers. Role functions like an engineering internship with concrete deliverables. The cabinet comprises the president, vice president and technical leads. Members are voted into these offices from a pool of nominees every semester. Additionally, the faculty advisor is considered an executive member of the organization with a permanent seat in the cabinet. All cabinet members get one vote towards the election of technical leads. The President is elected for a term of one semester from a pool of people that held the title of technical specialist or above for at least one semester. The President role is to lead the organization and help its members to achieve the proposed goals. Responsible for the operation of the organization, liaising with USC and departmental administration, and recruiting new members in addition to providing technical assistance where needed. The president is elected among nominated office bearers through general vote by all office bearing members. The president may also cast one extra recorded (non-anonymous, publicly disclosed) vote in any election resulting in a tie. Former presidents have the option to cast one vote towards the selection of projects and technical leads. There may only be one president for the organization at any time. The runner-up in the presidential election is nominated as Vice President. The vice president shares duties and responsibilities with the president including assisting in the leadership of the organization, and recruiting new members. Former vice presidents have the option to cast one vote towards the selection of projects and technical leads. There may only be one vice president for the organization at any time. Technical Leads are elected for one semester term from a pool of people that held the title of technical specialist or above for at least one semester. Technical Leads guids a team of Technical Specialists on a project to add features to or improve the track performance of the car. At the end of every semester, each team will publish an article/blog post as a tutorial and hold workshop sessions as needed with the participants. There may be more than one technical leader for a project. At the same time, a technical leader can lead more than one project. Technical leaders for a project are elected by a majority vote of the president, vice president and past cabinet members after nominations among office bearers. Office bearers may nominate themselves as candidates by proposing project ideas that they want to implement. Candidates will present their projected ideas for the projects they want to lead, which will form the basis for their election to office. A staff or faculty member of USC holds the position of faculty advisor. The faculty advisor oversees the operation of the organization and ensures that the Race On continues to comply with USC policies. There may be one or more faculty advisors for the organization.","title":"Article 2 \u2013 Office bearers and the organizing board"},{"location":"constitution/#article-3-elections","text":"Elections must happen for all cabinet offices in the week before the final race. The elections must be ratified by the faculty advisor(s) of the organization and the retiring cabinet members.","title":"Article 3 \u2013 Elections"},{"location":"constitution/#section-3a-eligibility-and-nominations","text":"Technical specialist candidates must have completed at least one semester as participants before applying for candidacy. (Online form application) All candidates for office bearer posts need to have completed at least one semester as a technical specialist to be eligible for nomination. Technical leads may be nominated on the basis of one or more new project ideas, improvements to old projects, or novel tracks in existing projects. Former and current presidents and vice presidents may nominate themselves as candidates for the same positions only if the entire current cabinet not including themselves unanimously approves of the nomination. Former president and vice president must ensure a smooth power transition and knowledge transfer to the new president and vice president.","title":"Section 3a: Eligibility and Nominations"},{"location":"constitution/#section-3b-voting","text":"During a voting cycle, the existing cabinet selects technical specialists from the list of candidates and assigns them to projects based on their preferences. Then, all current office bearers including technical specialists and the cabinet vote for the new president and vice president among the nominated candidates. Now, the newly formed cabinet votes on the new technical leads to be inducted into the cabinet as well as the project ideas that will be pursued for the upcoming semester. Here, former presidents and vice presidents of the organization each get one vote.","title":"Section 3b: Voting"},{"location":"constitution/#article-4-emergency-powers","text":"The faculty advisor(s) have full autonomous authority to override all or any part of the constitution including fill all offices including cabinet positions without the need for any approval so long as the goals of the organization as defined in the preamble are not breached. With approval from the faculty advisor(s), the current president and vice president may also fill the entire cabinet without elections in case vacancies of office need to be addressed.","title":"Article 4: Emergency powers"},{"location":"constitution/#article-5-projects","text":"Current cabinet members and new technical lead candidates can suggest new projects, additional technical tracks based on the proposed projects, or improvements to old projects. The list of projects must be made available on the Race On webpage and updated by the members of the organization. At least one project in each of the following tracks must be fulfilled each semester: Perception, Control, Planning, Simulation It is also crucial that at the end of the semester, the technical lead(s) spearheading the project write a report, technical paper, or blog post detailing their progress. Additionally, workshops and tutorials must be conducted as needed to disseminate the information among all members and participants of Race On.","title":"Article 5: Projects"},{"location":"constitution/#article-6-constitutional-committees","text":"First Constitutional committee (September 4, 2020) comprises Suraj Chakravarthi Raja with Valeriu Balaban, Fernando Valladares Monteiro, Jayson Sia, Amy Puente, Ali Marjaninejad and Thanos Rompokos.","title":"Article 6: Constitutional committees"},{"location":"localization/","text":"Real-Time Localization using AprilTags on a Raspberry Pi \u00b6 Date: September 4, 2020 Author: Theodore Lewitt Overview \u00b6 This article describes the work I did for the Race On RC self-driving car competition at USC. This code will be used by future competitors for car localization on the track! I will show you how to set-up both the hardware and software used on this project. I will also explain the reasoning behind each part of the main algorithm and how to make it run at 30+ FPS. Final Product \u00b6 ADD VIDEO System Requirements \u00b6 For each frame the camera sends to the Raspberry Pi, we need to process the data quickly enough to run in real time. For more concrete requirements, we can look at the servo motor on the car which can only update at 50 Hz. So we don't need anything faster than 50 FPS but will need greater than 20 FPS otherwise the car can travel too far in between updates. The other requirement is high localization accuracy so our car never is disqualified for going off the track. The white boundaries of the Race On track are 2 inches thick, so we ideally want accuracy up to 2 inches so our car can hug the track around the turns for the fastest time. However, most teams take a more conservative approach and we will require less than 5 inch error. FPS: 20 - 50, ideally >40 Max Localization Error: 5 inches (12.7 cm) What You'll Need \u00b6 Printed AprilTags, download [here] (https://github.com/AprilRobotics/apriltag-imgs/tree/master/tagStandard41h12), resize to 6 inches by 6 inches using software like GIMP , and print! Printed Checkerboard for camera calibration, download here and print! Raspberry Pi Wide Angle Camera Module Set-Up \u00b6 Install OpenCV on the Pi following this guide Clone this repo and install other dependencies using pip install -r requirements.txt Measure the size of your AprilTags in meters. ADD PHOTO HERE Camera Calibration \u00b6 Before we start using the AprilTags for localization, we need some more information about the camera. The Wide Angle Camera Module is a fisheye camera with 160 degrees of view, compared to 80\u201390 degrees on a normal camera. The trade-off with an fisheye camera is straight lines become curves near the edges of the image. ADD PHOTO We preform camera calibration to understand where a point in 3D space maps to the 2D pixel coordinates in the image. The output of the camera calibration is the intrinsic matrix K and the distortion array D . To learn some of the theory behind camera calibration and what K means, check out this blog . OpenCV has the fisheye calibration module, which finds both K and D from a set of images of a checkerboard. Use the fisheye_calibration.py script to obtain K and D . Scene Setup \u00b6 This is the final step before diving into the actual code, we need to place the printed AprilTags on a flat surface (wall, floor, ceiling,\u2026). Determine a global coordinate frame origin, ideally the bottom left corner of the room as it will make the following steps easier and keep all the numbers positive. From this point, we will use positive X to the right, positive Y going up and positive Z going out of the wall towards you. Place AprilTags around the room, in pairs of 2 or 3. The detections start failing from farther than 3 meters. Make sure the tag is not rotated or upside down, using a visualization script like visualize_frame.py . Measure the position of the tags, in inches from your global origin. Determine the orientation of the tags in reference to your global coordinate system in terms of Euler angles. Euler angles are a way to describe rotations in 3D and consist of 3 angles; X, Y and Z. To read more about Euler angles, I'd recommend this article . If your tag is on a wall, the only angle that will change is the Y angle. If your tag is on a floor or ceiling, the only angle that will change is the X angle. Save all of these measurements using the add_tag() function from tag.py , which takes the tag id, coordinates (in inches) and Euler angles (in radians). This function will automatically convert inches to meters for you. Getting Images from the Raspberry Pi \u00b6 The PiCamera module provides many ways to capture frames from the camera, but many don't provide enough FPS for our use case. For example, the capture_continuous function allows us to rapidly capture individual frames, but is limited to 20\u201330 FPS. Instead we record a video, and write to a custom class that can get up to 90 FPS. Our custom class, called Stream, is where all of the image processing and AprilTag detection takes place. This happens inside the main function write(), which receives raw YUV camera data from camera.start_recording() function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #Inside real_time.py with PiCamera() as camera: stream = Stream() try: # Start recording frames to the stream object camera.start_recording(stream, format='yuv') t0 = time() while True: camera.wait_recording() # If the time limit is reached, end the recording if (time() - t0) > MAX_TIME: camera.stop_recording() break Processing the Images \u00b6 The majority of the processing is within the write() function of the Stream class. There are 5 main steps that take an raw camera image to an measurement of the camera's location. 1. Raw Data to OpenCV Image 2. Undisortion 3. AprilTag Detection 4. Pose Estimation 5. Filtering Raw Data to OpenCV Image \u00b6 AprilTags detect on grayscale images, so instead of capturing a RGB image and converting to grayscale, we capture a YUV image. An YUV image is composed of three channels: luminance(Y), chroma blue (U), chroma red (V). The Y channel corresponds to a grayscale image, so when the camera writes to the stream, we only need to read in the first channel of the image to get the grayscale. OpenCV represents it's images as numpy arrays in Python. We use np.frombuffer() to read the bytes into a numpy array. 1 2 3 4 # Within stream.py, data is the raw bytes from the camera res = (640,480) I_raw = np.frombuffer(data, dtype=np.uint8, count=res[1]*res[0]) I_raw = I_raw.reshape(res[1], res[0]) Undistortion \u00b6 Once we have the raw image, we need to apply un-distortion to our image to reverse all of the fisheye distortion before detecting AprilTags. Un-distortion has two steps, one that can be pre-calculated and one that needs to be performed on each frame. The pre-calculated step is computing a rectification and un-distortion matrices using cv2.fisheye.initUndistortRectifyMap(). These matrices are inputted into the cv2.remap() function, which needs to be called on each frame and outputs the corrected image. 1 2 3 4 5 6 7 ### In real_time.py # K and D are camera intrinsics from the Camera Calibration # Res is the resolution of the image map_1, map_2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, res, cv2.CV_16SC2) ### In stream.py I = cv2.remap(I_raw, map_1, map_2) AprilTag Detection \u00b6 To use the python AprilTags bindings from this repo, you first create a Detector object. There are a bunch of different arguments for creating a Detector, and I'll highlight the most important ones. - Families: The families of AprilTags it should look for, we use tagStandard41h12. - nthreads: Number of threads to use for detection, using more will speed up detections. - quad-decimate: Uses a lower resolution image for detection, improving speed but reducing accuracy/pose estimation. Defaults to 1.0 but I'd recommend a value between 2.0 and 4.0 for optimal tradeoff. - debug: Defaults to 0 but if set to 1 will save images from each step in the detection process. Very computationally expensive, should only be used on single frames. To use the Detector, we call the detect() function on a image. For pose estimation, we set estimate_tag_pose to True and supply the tag size and four elements from the camera's K matrix. The output of the detect() functions is a list of Detection objects. Each Detection contains a lot of information about the tag it detected and I'll highlight the important ones. Tag_ID: The ID of the Tag Corners: A list of 4 tuples containing the pixel coordinates of the corners. These wrap counter clockwise from the bottom left corner. Pose_R: A 3x3 Rotation matrix of the tag in the camera frame Pose_T: A 3x1 Translation vector of the tag in the camera frame 1 2 3 4 5 6 7 ### In real_time.py detector = Detector(families=\"tagStandard41h12\",nthreads=4) ### In stream.py PARAMS = [K[0,0],K[1,1],K[2,0],K[2,1]] #elements from the K matrix TAG_SIZE = .10 #Tag size from Step 1 in meters detections = detector.detect(I, estimate_tag_pose=True, camera_params=PARAMS,tag_size=TAG_SIZE) Pose Estimation \u00b6 We now have all of the information we need to estimate the position of the camera. What we want is the position of the camera in the global frame, and we can break that into two steps. We get the location of the camera in the tag frame and combine with the location of the tag in the global frame. First, we need the rotation matrix R and translation vector t of the camera in the tag frame. We can get these by inverting the Pose_R and Pose_T we get from the Apriltags. R is an orthogonal rotation matrix, meaning the inverse is the same as the transpose. t is a translation vector, so its inverse just negates everything. \\(R_camera_tag = Pose_R^{T}\\) \\(t_camera_tag = -1 * Pose_t\\) To get the exact location of the camera in the tag frame, we start with the tag center in the tag frame, translate by t_camera_tag and multiply by R_camera_tag . But the tag center in the tag frame is (0,0,0) so our final equation is \\(Position_tag = R_camera_tag * t_camera_tag\\) The next step is a correction between the official tag frame on the AprilTags website and the tag frame that I chose to use. My reasoning for choosing to use a slightly different tag frame is that my tag frame aligns with the global frame if all the Euler angles are 0 which helps me visualize the global transformations. The official AprilTags tag frame has the z-axis pointing from the tag center into the wall. The x-axis is to the right in the image taken by the camera, and y is down. This means to transform to our global frame with X right, Y up and Z out of the wall we need X->X,Y->-Y and Z->-Z. We can use a 3x3 diagonal permutation matrix P to model this, with either 1 or -1 on the diagonal elements. ADD PHOTO \\(Position_tag_unofficial = P * Position_tag\\) Now we go from the tag frame to the global frame using the information from the look-up table we populated when doing the scene setup. We have our global rotation matrix \\(R_{g}\\) and global translation vector \\(t_{g}\\) . \\(Position_global = R_{g} * Position_tag_unofficial + t_{g}\\) In Python, we can use the matrix mulptiplcation symbol @ introduced in 3.6. Combining all of the transformations becomes 1 2 unofficial_tag_position = P @ Pose_R.T @ (-1 * Pose_T) global_position = R_g @ unofficial_tag_position + t_g If there are multiple tags in a single frame, we estimate the position using all the tags, and then average the estimates. Filtering \u00b6 We now have a pose estimate of the camera position, but how accurate is this measurement? In my work, I've found the Apriltags are accurate to \u00b1 10 cm when viewed close to straight on and from closer than 2 meters. We do have a lot more information we can use to improve these measurements. First, we can assume the camera won't move too drastically between frames, so we can use a moving average to smooth out some of the noise from frame to frame. The parameter choice here is how many time steps to average, with more time steps resulting in a more smooth trajectory over time. Now, with a little more information about where our camera should be, for example if we know the inputs to our self-driving car's motor at the previous frame, we can use a g-h filter or a Kalman filter to further improve our position accuracy. There is a lifetime of really interesting work in this field and one of the best intro books I've read is here . 1 pose = moving_average(global_position,previous_positions,num_time_steps) Putting it all together \u00b6 Here is psuedocode for real_time.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #Camera Calibration Parameters camera_info = load_camera_information() #Scene Setup tags=Tag() tags.add_tag(tag_id,x,y,z,theta_x,theta_y,theta_z) ### Detector detector = Detector(families=\"tagStandard41h12\",nthreads=4) #Main Loop with PiCamera() as camera: stream = Stream(tags,camera_info,detector) try: # Start recording frames to the stream object camera.start_recording(stream, format='yuv') t0 = time() while True: camera.wait_recording() # If the time limit is reached, end the recording if (time() - t0) > MAX_TIME: camera.stop_recording() break Psuedocode for stream.py 1 2 3 4 5 6 def write(raw_bytes): I_raw = np.frombuffer(raw_bytes) I = undistort(I_raw) detected_tags = detector.detect(I) raw_position_estimate = estimate_pose(detected_tags) smoothed_position = moving_average(raw_position_estimate)","title":"Real-Time Localization using AprilTags on a Raspberry Pi"},{"location":"localization/#real-time-localization-using-apriltags-on-a-raspberry-pi","text":"Date: September 4, 2020 Author: Theodore Lewitt","title":"Real-Time Localization using AprilTags on a Raspberry Pi"},{"location":"localization/#overview","text":"This article describes the work I did for the Race On RC self-driving car competition at USC. This code will be used by future competitors for car localization on the track! I will show you how to set-up both the hardware and software used on this project. I will also explain the reasoning behind each part of the main algorithm and how to make it run at 30+ FPS.","title":"Overview"},{"location":"localization/#final-product","text":"ADD VIDEO","title":"Final Product"},{"location":"localization/#system-requirements","text":"For each frame the camera sends to the Raspberry Pi, we need to process the data quickly enough to run in real time. For more concrete requirements, we can look at the servo motor on the car which can only update at 50 Hz. So we don't need anything faster than 50 FPS but will need greater than 20 FPS otherwise the car can travel too far in between updates. The other requirement is high localization accuracy so our car never is disqualified for going off the track. The white boundaries of the Race On track are 2 inches thick, so we ideally want accuracy up to 2 inches so our car can hug the track around the turns for the fastest time. However, most teams take a more conservative approach and we will require less than 5 inch error. FPS: 20 - 50, ideally >40 Max Localization Error: 5 inches (12.7 cm)","title":"System Requirements"},{"location":"localization/#what-youll-need","text":"Printed AprilTags, download [here] (https://github.com/AprilRobotics/apriltag-imgs/tree/master/tagStandard41h12), resize to 6 inches by 6 inches using software like GIMP , and print! Printed Checkerboard for camera calibration, download here and print! Raspberry Pi Wide Angle Camera Module","title":"What You'll Need"},{"location":"localization/#set-up","text":"Install OpenCV on the Pi following this guide Clone this repo and install other dependencies using pip install -r requirements.txt Measure the size of your AprilTags in meters. ADD PHOTO HERE","title":"Set-Up"},{"location":"localization/#camera-calibration","text":"Before we start using the AprilTags for localization, we need some more information about the camera. The Wide Angle Camera Module is a fisheye camera with 160 degrees of view, compared to 80\u201390 degrees on a normal camera. The trade-off with an fisheye camera is straight lines become curves near the edges of the image. ADD PHOTO We preform camera calibration to understand where a point in 3D space maps to the 2D pixel coordinates in the image. The output of the camera calibration is the intrinsic matrix K and the distortion array D . To learn some of the theory behind camera calibration and what K means, check out this blog . OpenCV has the fisheye calibration module, which finds both K and D from a set of images of a checkerboard. Use the fisheye_calibration.py script to obtain K and D .","title":"Camera Calibration"},{"location":"localization/#scene-setup","text":"This is the final step before diving into the actual code, we need to place the printed AprilTags on a flat surface (wall, floor, ceiling,\u2026). Determine a global coordinate frame origin, ideally the bottom left corner of the room as it will make the following steps easier and keep all the numbers positive. From this point, we will use positive X to the right, positive Y going up and positive Z going out of the wall towards you. Place AprilTags around the room, in pairs of 2 or 3. The detections start failing from farther than 3 meters. Make sure the tag is not rotated or upside down, using a visualization script like visualize_frame.py . Measure the position of the tags, in inches from your global origin. Determine the orientation of the tags in reference to your global coordinate system in terms of Euler angles. Euler angles are a way to describe rotations in 3D and consist of 3 angles; X, Y and Z. To read more about Euler angles, I'd recommend this article . If your tag is on a wall, the only angle that will change is the Y angle. If your tag is on a floor or ceiling, the only angle that will change is the X angle. Save all of these measurements using the add_tag() function from tag.py , which takes the tag id, coordinates (in inches) and Euler angles (in radians). This function will automatically convert inches to meters for you.","title":"Scene Setup"},{"location":"localization/#getting-images-from-the-raspberry-pi","text":"The PiCamera module provides many ways to capture frames from the camera, but many don't provide enough FPS for our use case. For example, the capture_continuous function allows us to rapidly capture individual frames, but is limited to 20\u201330 FPS. Instead we record a video, and write to a custom class that can get up to 90 FPS. Our custom class, called Stream, is where all of the image processing and AprilTag detection takes place. This happens inside the main function write(), which receives raw YUV camera data from camera.start_recording() function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #Inside real_time.py with PiCamera() as camera: stream = Stream() try: # Start recording frames to the stream object camera.start_recording(stream, format='yuv') t0 = time() while True: camera.wait_recording() # If the time limit is reached, end the recording if (time() - t0) > MAX_TIME: camera.stop_recording() break","title":"Getting Images from the Raspberry Pi"},{"location":"localization/#processing-the-images","text":"The majority of the processing is within the write() function of the Stream class. There are 5 main steps that take an raw camera image to an measurement of the camera's location. 1. Raw Data to OpenCV Image 2. Undisortion 3. AprilTag Detection 4. Pose Estimation 5. Filtering","title":"Processing the Images"},{"location":"localization/#raw-data-to-opencv-image","text":"AprilTags detect on grayscale images, so instead of capturing a RGB image and converting to grayscale, we capture a YUV image. An YUV image is composed of three channels: luminance(Y), chroma blue (U), chroma red (V). The Y channel corresponds to a grayscale image, so when the camera writes to the stream, we only need to read in the first channel of the image to get the grayscale. OpenCV represents it's images as numpy arrays in Python. We use np.frombuffer() to read the bytes into a numpy array. 1 2 3 4 # Within stream.py, data is the raw bytes from the camera res = (640,480) I_raw = np.frombuffer(data, dtype=np.uint8, count=res[1]*res[0]) I_raw = I_raw.reshape(res[1], res[0])","title":"Raw Data to OpenCV Image"},{"location":"localization/#undistortion","text":"Once we have the raw image, we need to apply un-distortion to our image to reverse all of the fisheye distortion before detecting AprilTags. Un-distortion has two steps, one that can be pre-calculated and one that needs to be performed on each frame. The pre-calculated step is computing a rectification and un-distortion matrices using cv2.fisheye.initUndistortRectifyMap(). These matrices are inputted into the cv2.remap() function, which needs to be called on each frame and outputs the corrected image. 1 2 3 4 5 6 7 ### In real_time.py # K and D are camera intrinsics from the Camera Calibration # Res is the resolution of the image map_1, map_2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, res, cv2.CV_16SC2) ### In stream.py I = cv2.remap(I_raw, map_1, map_2)","title":"Undistortion"},{"location":"localization/#apriltag-detection","text":"To use the python AprilTags bindings from this repo, you first create a Detector object. There are a bunch of different arguments for creating a Detector, and I'll highlight the most important ones. - Families: The families of AprilTags it should look for, we use tagStandard41h12. - nthreads: Number of threads to use for detection, using more will speed up detections. - quad-decimate: Uses a lower resolution image for detection, improving speed but reducing accuracy/pose estimation. Defaults to 1.0 but I'd recommend a value between 2.0 and 4.0 for optimal tradeoff. - debug: Defaults to 0 but if set to 1 will save images from each step in the detection process. Very computationally expensive, should only be used on single frames. To use the Detector, we call the detect() function on a image. For pose estimation, we set estimate_tag_pose to True and supply the tag size and four elements from the camera's K matrix. The output of the detect() functions is a list of Detection objects. Each Detection contains a lot of information about the tag it detected and I'll highlight the important ones. Tag_ID: The ID of the Tag Corners: A list of 4 tuples containing the pixel coordinates of the corners. These wrap counter clockwise from the bottom left corner. Pose_R: A 3x3 Rotation matrix of the tag in the camera frame Pose_T: A 3x1 Translation vector of the tag in the camera frame 1 2 3 4 5 6 7 ### In real_time.py detector = Detector(families=\"tagStandard41h12\",nthreads=4) ### In stream.py PARAMS = [K[0,0],K[1,1],K[2,0],K[2,1]] #elements from the K matrix TAG_SIZE = .10 #Tag size from Step 1 in meters detections = detector.detect(I, estimate_tag_pose=True, camera_params=PARAMS,tag_size=TAG_SIZE)","title":"AprilTag Detection"},{"location":"localization/#pose-estimation","text":"We now have all of the information we need to estimate the position of the camera. What we want is the position of the camera in the global frame, and we can break that into two steps. We get the location of the camera in the tag frame and combine with the location of the tag in the global frame. First, we need the rotation matrix R and translation vector t of the camera in the tag frame. We can get these by inverting the Pose_R and Pose_T we get from the Apriltags. R is an orthogonal rotation matrix, meaning the inverse is the same as the transpose. t is a translation vector, so its inverse just negates everything. \\(R_camera_tag = Pose_R^{T}\\) \\(t_camera_tag = -1 * Pose_t\\) To get the exact location of the camera in the tag frame, we start with the tag center in the tag frame, translate by t_camera_tag and multiply by R_camera_tag . But the tag center in the tag frame is (0,0,0) so our final equation is \\(Position_tag = R_camera_tag * t_camera_tag\\) The next step is a correction between the official tag frame on the AprilTags website and the tag frame that I chose to use. My reasoning for choosing to use a slightly different tag frame is that my tag frame aligns with the global frame if all the Euler angles are 0 which helps me visualize the global transformations. The official AprilTags tag frame has the z-axis pointing from the tag center into the wall. The x-axis is to the right in the image taken by the camera, and y is down. This means to transform to our global frame with X right, Y up and Z out of the wall we need X->X,Y->-Y and Z->-Z. We can use a 3x3 diagonal permutation matrix P to model this, with either 1 or -1 on the diagonal elements. ADD PHOTO \\(Position_tag_unofficial = P * Position_tag\\) Now we go from the tag frame to the global frame using the information from the look-up table we populated when doing the scene setup. We have our global rotation matrix \\(R_{g}\\) and global translation vector \\(t_{g}\\) . \\(Position_global = R_{g} * Position_tag_unofficial + t_{g}\\) In Python, we can use the matrix mulptiplcation symbol @ introduced in 3.6. Combining all of the transformations becomes 1 2 unofficial_tag_position = P @ Pose_R.T @ (-1 * Pose_T) global_position = R_g @ unofficial_tag_position + t_g If there are multiple tags in a single frame, we estimate the position using all the tags, and then average the estimates.","title":"Pose Estimation"},{"location":"localization/#filtering","text":"We now have a pose estimate of the camera position, but how accurate is this measurement? In my work, I've found the Apriltags are accurate to \u00b1 10 cm when viewed close to straight on and from closer than 2 meters. We do have a lot more information we can use to improve these measurements. First, we can assume the camera won't move too drastically between frames, so we can use a moving average to smooth out some of the noise from frame to frame. The parameter choice here is how many time steps to average, with more time steps resulting in a more smooth trajectory over time. Now, with a little more information about where our camera should be, for example if we know the inputs to our self-driving car's motor at the previous frame, we can use a g-h filter or a Kalman filter to further improve our position accuracy. There is a lifetime of really interesting work in this field and one of the best intro books I've read is here . 1 pose = moving_average(global_position,previous_positions,num_time_steps)","title":"Filtering"},{"location":"localization/#putting-it-all-together","text":"Here is psuedocode for real_time.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #Camera Calibration Parameters camera_info = load_camera_information() #Scene Setup tags=Tag() tags.add_tag(tag_id,x,y,z,theta_x,theta_y,theta_z) ### Detector detector = Detector(families=\"tagStandard41h12\",nthreads=4) #Main Loop with PiCamera() as camera: stream = Stream(tags,camera_info,detector) try: # Start recording frames to the stream object camera.start_recording(stream, format='yuv') t0 = time() while True: camera.wait_recording() # If the time limit is reached, end the recording if (time() - t0) > MAX_TIME: camera.stop_recording() break Psuedocode for stream.py 1 2 3 4 5 6 def write(raw_bytes): I_raw = np.frombuffer(raw_bytes) I = undistort(I_raw) detected_tags = detector.detect(I) raw_position_estimate = estimate_pose(detected_tags) smoothed_position = moving_average(raw_position_estimate)","title":"Putting it all together"},{"location":"projects/","text":"Race On 2020 Projects \u00b6 Project Deliverables \u00b6 Each project must include the following deliverables: Demo Video \u2013 a short video showing the project in action along with a brief description and the result for different values of the configuration parameters. Bio of the team members who worked on the project. An article/blog post \u2013 brief description of the technologies used and they connect together, detailed description of the project result and source code, examples how to use and description of the most important parameters and how to select them. Instructions, how to add to existing work, detailed performance analysis if applicable, future steps or direction that might yield better performance. Bio of the team members who worked on the project and their contact information. Any additional requirements stated in the project description High Priority Projects \u00b6 Pose estimation using visual tags \u00b6 Topic : Perception Priority : High Difficulty : Low Detecting the track ahead of the car is important for local steering decisions but might compromise the overall performance on the track. For example, when detecting a turn without knowing the map, the driving algorithm must assume the worst case scenario which could result in a significant decrease in speed. However, by knowing the map, the best speed for each turn can be used to drastically improve the performance. For that, special tags such as Apriltags 3 can be placed on the track. Upon detection of these tags, the car position, orientation and speed can be inferred. The goal of this project is to evaluate Apriltags 3 and other tags in terms of reliability of detection and required real-time CPU usage. Moreover, different placement strategies for those tags can also be evaluated. References: AprilTags Github Alvar tags ROS page Chilitags Github AprilTags ROS package Additional deliverables: Tags to be used and their placement strategy Localization using visual tags \u00b6 Topic : Perception Priority : High Difficulty : Medium Real cars have the advantage of using GPS for localization, but this is not the case for Race On cars. Knowing the exact position of the car on the track offers a big advantage as it allows a more precise path planning which heavily outperforms the stay-in-the-middle strategy. By placing two or three Apriltags 3 or other similar tags on each segment, an accurate track map can be constructed and used for precise localization of the car. Moreover, this localization can be also used in estimating the car model parameters of the state-space controller. References: Mobile Robot Localization, Chapter 7 Different localization algorithms Some insight on choosing AprilTags for localization Similar project source code (in chinese) Frequency domain model estimation and control \u00b6 Topic : Control Priority : High Difficulty : Medium The main goal of this project is to gain a deeper understanding of the system\u2019s frequency response and apply it to design controllers such as PID and lead-lag. This will provide teams more formal tools to tune their vehicle gains. We assume the vehicle lateral dynamics can be modelled as a second order system. The first step is to estimate its parameters (damping ratio and natural frequency), which will most likely depend on the longitudinal velocity. Then, the controller gains can be determined based on current velocity and desired system response. Additional deliverables Brief introduction frequency domain models, why they are useful for Race On and mathematical definition of the model being used Standard procedure to estimate model parameters Sample procedure to determine system gains based on analytical and graphical control tools Average lap times of different controllers Time domain model estimation and control \u00b6 Topic : Control Priority : High Difficulty : High This project has two main goals. The first is to develop a reliable procedure to estimate vehicle parameters necessary for state-space control approaches. The second is to use the state-space model for the design of a state-feedback control, which might also include a feedforward term. We start by assuming the vehicle can be modelled by a two-wheel bicycle model. Next, we will provide a standard procedure that any participant can follow to obtain all the physical parameters of the model. This phase includes some literature research and creativity to adapt the methods to the Race On environment. Then, a state-observer is necessary to estimate in real-time the system states, which will be used by the controller. The last step is to use the estimated states in a state-feedback controller. If time allows, this project can go on to more advanced observer/ controller design such as Kalman filtering, LQR and MPC. Additional deliverables: Brief introduction to time domain models, state-space notation, why they are useful for Race On and mathematical definition of the model being used Standard procedure to estimate model parameters Blog-post with brief introduction to observers and how to apply it to the Race On vehicle Description of controller, including state-feedback and possible feedforward term. Sample procedure on defining gains Average lap times of different controllers State-space controller \u00b6 Topic : Control Priority : High Difficulty : High The main goal of this project is to implement an advanced controller design by utilizing the car dynamical model so as to achieve a better racing performance. This project can be subdivided into the car model estimation and control and state observer design. First, the car model must be estimated either using parametric (first principles model) and/or non-parametric (data-driven) methods. Second, a state-observer design is necessary to estimate the hidden state dynamics used for the higher level control. A typical implementation is the Extended Kalman Filter. Finally, given the model and observer designs, advanced control techniques (e.g. LQR, MPC, etc.) will be implemented and compared with the typical PID implementation. Comparisons in performance will be evaluated. References: Paper on LQR controller design for drifting an 1/10 RC car Thesis on LQR controller design for RC car MPC controller paper MPC controller website Kalman filter python library + book explaining the library Reference controller implementation on a 1/10 car (BARC source code) Simulation and Development Docker images \u00b6 Topic : Simulation Priority : High Difficulty : Low In order to ensure a smooth collaboration, the same development environment must be used by all parties. The simplest way to achieve this is to use Docker images (lightweight virtual machines). The goal is to create two such images, one for simulation and one for development. The simulation image will be used by GitHub to run the code at each commit and check it\u2019s output. The image contains all the necessary tools to simulate a Race On environment based on Gazebo and ROS. The development image is based on the simulation image but in addition to that contains tools to develop code. This image will be used by all technical leaders and specialists when working on the project. This approach will facilitate integrating each team\u2019s work into the Race On project. Additional deliverables: Docker files for the two images Reliable Camera Calibration and Bird\u2019s Eye view image mapping \u00b6 Topic : Perception Priority : High Difficulty : Low Could not load the video file. Check reference 1 below for more details. Wide angle cameras introduce distortions that increase as we go farther from the camera center. These distortions make straight lines look curved. This is a big problem and introduces errors when performing visual odometry or visual localization which rely on a calibrated camera. Therefore, camera calibration must be the first step for any visual algorithm. The goal of this project is to design a calibration procedure that is easy to implement and can be used by teams at any time if they suspect that the camera is not calibrated. References: Apriltag based camera calibration Camera calibration explanation + code Bird\u2019s eye view explanation + code Rolling Shutter Calibration + IMU based on camera + papers Medium Priority Projects \u00b6 Visual Odometry \u00b6 Topic : Perception Priority : Medium Difficulty : Medium Visual odometry is the problem of estimating the trajectory of the camera only from its image feed. In more details, given two images we want to estimate the translation over x, y, z and the rotation \ud835\udf03x, \ud835\udf03y, \ud835\udf03z that if applied will transform the first image into the second one. This is a much simpler problem than localization, although the two are connected with the difference that in localization the position is absolute whereas in odometry the position is relative. References: Tutorial An overview of image based localization Similar project Rapidly Exploring Random Trees for Path Planning \u00b6 Topic : Planning Priority : Medium Difficulty : Low Rapidly Exploring Random Trees is an algorithm to search for an optimal trajectory on a constrained map (keeping the car inside the track boundaries). The random part for generating trajectories can be biased towards a trajectory with desirable characteristics (higher car speed and margin of error). The goal of the project is to implement RRT and RRT* or other variants that receive a track configuration and output an optimal trajectory for the car to complete a lap. The algorithm must be configurable to allow a trade off between car speed and margin of error. As a bonus task, one could integrate the car dynamics model into the RRT algorithm. References: Explanation Source code + other similar algorithms Vehicle Path Planning paper Integration with Webviz \u00b6 Topic : Simulation Priority : Medium Difficulty : Low Webviz is a web-based application that can listen to ROS messages and visualize them in real-time or playback from a rosbag. The goal is to integrate webviz with the Race On starter code and explore different visualization templates and propose a better solution given the Race On problem. In addition, webviz can publish ROS messages thus enabling the design of a dashboard for car control. References: Webviz Github page Additional deliverables: Dashboard configuration Low Priority Projects \u00b6 Port existing code to ROS 2 and Real-Time Linux \u00b6 Topic : Other Priority : Low Difficulty : High Currently, Race On starter code uses ROS 1 and heavily relies on Python 2.7 which as of 2020 is obsolete. ROS 2 and Python 3 is the way to go in the long run. The goal is to port the existing code to ROS 2 and Python 3. Moreover, Linux kernel was not designed for Real-Time applications but there exists a patch that fixes this. This kernel makes the Raspberry Pi slower by at most 10% but instead guarantees that parts of the program will be executed on time if configured so. The goal is to patch a Pi image and provide instruction on how to set up the ROS code to benefit from the real-time kernel. Additional Reference Materials \u00b6 https://f1tenth.org/learn.html https://github.com/SonamYeshe/ENPM673-Perception-for-Autonomous-Robots Various Robotics Algorithms implemented in Python","title":"Projects"},{"location":"projects/#race-on-2020-projects","text":"","title":"Race On 2020 Projects"},{"location":"projects/#project-deliverables","text":"Each project must include the following deliverables: Demo Video \u2013 a short video showing the project in action along with a brief description and the result for different values of the configuration parameters. Bio of the team members who worked on the project. An article/blog post \u2013 brief description of the technologies used and they connect together, detailed description of the project result and source code, examples how to use and description of the most important parameters and how to select them. Instructions, how to add to existing work, detailed performance analysis if applicable, future steps or direction that might yield better performance. Bio of the team members who worked on the project and their contact information. Any additional requirements stated in the project description","title":"Project Deliverables"},{"location":"projects/#high-priority-projects","text":"","title":"High Priority Projects"},{"location":"projects/#pose-estimation-using-visual-tags","text":"Topic : Perception Priority : High Difficulty : Low Detecting the track ahead of the car is important for local steering decisions but might compromise the overall performance on the track. For example, when detecting a turn without knowing the map, the driving algorithm must assume the worst case scenario which could result in a significant decrease in speed. However, by knowing the map, the best speed for each turn can be used to drastically improve the performance. For that, special tags such as Apriltags 3 can be placed on the track. Upon detection of these tags, the car position, orientation and speed can be inferred. The goal of this project is to evaluate Apriltags 3 and other tags in terms of reliability of detection and required real-time CPU usage. Moreover, different placement strategies for those tags can also be evaluated. References: AprilTags Github Alvar tags ROS page Chilitags Github AprilTags ROS package Additional deliverables: Tags to be used and their placement strategy","title":"Pose estimation using visual tags"},{"location":"projects/#localization-using-visual-tags","text":"Topic : Perception Priority : High Difficulty : Medium Real cars have the advantage of using GPS for localization, but this is not the case for Race On cars. Knowing the exact position of the car on the track offers a big advantage as it allows a more precise path planning which heavily outperforms the stay-in-the-middle strategy. By placing two or three Apriltags 3 or other similar tags on each segment, an accurate track map can be constructed and used for precise localization of the car. Moreover, this localization can be also used in estimating the car model parameters of the state-space controller. References: Mobile Robot Localization, Chapter 7 Different localization algorithms Some insight on choosing AprilTags for localization Similar project source code (in chinese)","title":"Localization using visual tags"},{"location":"projects/#frequency-domain-model-estimation-and-control","text":"Topic : Control Priority : High Difficulty : Medium The main goal of this project is to gain a deeper understanding of the system\u2019s frequency response and apply it to design controllers such as PID and lead-lag. This will provide teams more formal tools to tune their vehicle gains. We assume the vehicle lateral dynamics can be modelled as a second order system. The first step is to estimate its parameters (damping ratio and natural frequency), which will most likely depend on the longitudinal velocity. Then, the controller gains can be determined based on current velocity and desired system response. Additional deliverables Brief introduction frequency domain models, why they are useful for Race On and mathematical definition of the model being used Standard procedure to estimate model parameters Sample procedure to determine system gains based on analytical and graphical control tools Average lap times of different controllers","title":"Frequency domain model estimation and control"},{"location":"projects/#time-domain-model-estimation-and-control","text":"Topic : Control Priority : High Difficulty : High This project has two main goals. The first is to develop a reliable procedure to estimate vehicle parameters necessary for state-space control approaches. The second is to use the state-space model for the design of a state-feedback control, which might also include a feedforward term. We start by assuming the vehicle can be modelled by a two-wheel bicycle model. Next, we will provide a standard procedure that any participant can follow to obtain all the physical parameters of the model. This phase includes some literature research and creativity to adapt the methods to the Race On environment. Then, a state-observer is necessary to estimate in real-time the system states, which will be used by the controller. The last step is to use the estimated states in a state-feedback controller. If time allows, this project can go on to more advanced observer/ controller design such as Kalman filtering, LQR and MPC. Additional deliverables: Brief introduction to time domain models, state-space notation, why they are useful for Race On and mathematical definition of the model being used Standard procedure to estimate model parameters Blog-post with brief introduction to observers and how to apply it to the Race On vehicle Description of controller, including state-feedback and possible feedforward term. Sample procedure on defining gains Average lap times of different controllers","title":"Time domain model estimation and control"},{"location":"projects/#state-space-controller","text":"Topic : Control Priority : High Difficulty : High The main goal of this project is to implement an advanced controller design by utilizing the car dynamical model so as to achieve a better racing performance. This project can be subdivided into the car model estimation and control and state observer design. First, the car model must be estimated either using parametric (first principles model) and/or non-parametric (data-driven) methods. Second, a state-observer design is necessary to estimate the hidden state dynamics used for the higher level control. A typical implementation is the Extended Kalman Filter. Finally, given the model and observer designs, advanced control techniques (e.g. LQR, MPC, etc.) will be implemented and compared with the typical PID implementation. Comparisons in performance will be evaluated. References: Paper on LQR controller design for drifting an 1/10 RC car Thesis on LQR controller design for RC car MPC controller paper MPC controller website Kalman filter python library + book explaining the library Reference controller implementation on a 1/10 car (BARC source code)","title":"State-space controller"},{"location":"projects/#simulation-and-development-docker-images","text":"Topic : Simulation Priority : High Difficulty : Low In order to ensure a smooth collaboration, the same development environment must be used by all parties. The simplest way to achieve this is to use Docker images (lightweight virtual machines). The goal is to create two such images, one for simulation and one for development. The simulation image will be used by GitHub to run the code at each commit and check it\u2019s output. The image contains all the necessary tools to simulate a Race On environment based on Gazebo and ROS. The development image is based on the simulation image but in addition to that contains tools to develop code. This image will be used by all technical leaders and specialists when working on the project. This approach will facilitate integrating each team\u2019s work into the Race On project. Additional deliverables: Docker files for the two images","title":"Simulation and Development Docker images"},{"location":"projects/#reliable-camera-calibration-and-birds-eye-view-image-mapping","text":"Topic : Perception Priority : High Difficulty : Low Could not load the video file. Check reference 1 below for more details. Wide angle cameras introduce distortions that increase as we go farther from the camera center. These distortions make straight lines look curved. This is a big problem and introduces errors when performing visual odometry or visual localization which rely on a calibrated camera. Therefore, camera calibration must be the first step for any visual algorithm. The goal of this project is to design a calibration procedure that is easy to implement and can be used by teams at any time if they suspect that the camera is not calibrated. References: Apriltag based camera calibration Camera calibration explanation + code Bird\u2019s eye view explanation + code Rolling Shutter Calibration + IMU based on camera + papers","title":"Reliable Camera Calibration and Bird\u2019s Eye view image mapping"},{"location":"projects/#medium-priority-projects","text":"","title":"Medium Priority Projects"},{"location":"projects/#visual-odometry","text":"Topic : Perception Priority : Medium Difficulty : Medium Visual odometry is the problem of estimating the trajectory of the camera only from its image feed. In more details, given two images we want to estimate the translation over x, y, z and the rotation \ud835\udf03x, \ud835\udf03y, \ud835\udf03z that if applied will transform the first image into the second one. This is a much simpler problem than localization, although the two are connected with the difference that in localization the position is absolute whereas in odometry the position is relative. References: Tutorial An overview of image based localization Similar project","title":"Visual Odometry"},{"location":"projects/#rapidly-exploring-random-trees-for-path-planning","text":"Topic : Planning Priority : Medium Difficulty : Low Rapidly Exploring Random Trees is an algorithm to search for an optimal trajectory on a constrained map (keeping the car inside the track boundaries). The random part for generating trajectories can be biased towards a trajectory with desirable characteristics (higher car speed and margin of error). The goal of the project is to implement RRT and RRT* or other variants that receive a track configuration and output an optimal trajectory for the car to complete a lap. The algorithm must be configurable to allow a trade off between car speed and margin of error. As a bonus task, one could integrate the car dynamics model into the RRT algorithm. References: Explanation Source code + other similar algorithms Vehicle Path Planning paper","title":"Rapidly Exploring Random Trees for Path Planning"},{"location":"projects/#integration-with-webviz","text":"Topic : Simulation Priority : Medium Difficulty : Low Webviz is a web-based application that can listen to ROS messages and visualize them in real-time or playback from a rosbag. The goal is to integrate webviz with the Race On starter code and explore different visualization templates and propose a better solution given the Race On problem. In addition, webviz can publish ROS messages thus enabling the design of a dashboard for car control. References: Webviz Github page Additional deliverables: Dashboard configuration","title":"Integration with Webviz"},{"location":"projects/#low-priority-projects","text":"","title":"Low Priority Projects"},{"location":"projects/#port-existing-code-to-ros-2-and-real-time-linux","text":"Topic : Other Priority : Low Difficulty : High Currently, Race On starter code uses ROS 1 and heavily relies on Python 2.7 which as of 2020 is obsolete. ROS 2 and Python 3 is the way to go in the long run. The goal is to port the existing code to ROS 2 and Python 3. Moreover, Linux kernel was not designed for Real-Time applications but there exists a patch that fixes this. This kernel makes the Raspberry Pi slower by at most 10% but instead guarantees that parts of the program will be executed on time if configured so. The goal is to patch a Pi image and provide instruction on how to set up the ROS code to benefit from the real-time kernel.","title":"Port existing code to ROS 2 and Real-Time Linux"},{"location":"projects/#additional-reference-materials","text":"https://f1tenth.org/learn.html https://github.com/SonamYeshe/ENPM673-Perception-for-Autonomous-Robots Various Robotics Algorithms implemented in Python","title":"Additional Reference Materials"},{"location":"registration/","text":"Fall 2020 Registration \u00b6 To register as a Technical Lead or Technical Specialist , please fill out the registration form below. When registering use your USC email . The form is open until midnight on Wednesday, September 9 (deadline updated). We will contact you after the deadline to discuss the next steps. For any queries, please contact Valeriu Balaban at vbalaban@usc.edu Loading\u2026 After submission of the form, please scroll up to see the confirmation message. You should also receive an email confirmation. If the form above is not loading use the following link .","title":"Registration"},{"location":"registration/#fall-2020-registration","text":"To register as a Technical Lead or Technical Specialist , please fill out the registration form below. When registering use your USC email . The form is open until midnight on Wednesday, September 9 (deadline updated). We will contact you after the deadline to discuss the next steps. For any queries, please contact Valeriu Balaban at vbalaban@usc.edu Loading\u2026 After submission of the form, please scroll up to see the confirmation message. You should also receive an email confirmation. If the form above is not loading use the following link .","title":"Fall 2020 Registration"},{"location":"rules/","text":"Rules \u2013 Spring 2020 \u00b6 Registration \u00b6 Registration for the Spring 2020 edition is open until Sunday, January 19 . Teams that register must pay $250 materials fee if all members are new to Race On. Teams where one or more team members competed in a previous Race On competition still have to register online but do not pay the materials fee. We encourage teams to reuse previously received cars and we can help replace certain components. Teams are formed of two to four students that are currently enrolled at USC and are the only ones who build, program, and race the car. Each team delegates a contact person that will represent the team. All teams compete in the same races and on the same racing track irrespective of their program and year of study. Students are required to exhibit appropriate sportsmanship behavior and any attempts of cheating would result in penalties or disqualification. Races \u00b6 Race On competition consists of three races with mandatory participation. Date Room Race 0 Saturday, February 8 EEB 132 Race 1 Saturday, February 29 EEB 132 Race 2 Friday, April 17 EEB 132 Teams are awarded points based on their performance in each race. In Race 0, teams will race on a simple loop track. The goal of this race is to stimulate teams to finish assembling their cars. In Race 1, teams will race on an average complexity track and will be awarded points for both participation and ranking. In Race 2 teams will race on a longer and more complex track. Teams will receive points for completing a lap and the ranking points will be double compared to the first race. Points \u00b6 Note The point system is designed such that a team that did average in Race 1 can still win the competition if they improve for Race 2. The best team from Race 1 does not automatically win if they do average in Race 2. However, winning Race 1 helps break some ties in the final standings. In all three races, teams will be awarded points for running the car, finishing a lap, and ranking in the top five in each race. Race 0 Race 1 Race 2 Run 2 2 - Lap - 2 4 Rank 1 - 10 20 Rank 2 - 8 16 Rank 3 - 6 12 Rank 4 - 4 8 Rank 5 - 2 4 Run points will be awarded in Race 0 and Race 1. Teams that manage to run their car, car moves autonomously on the track, will receive 2 points. Lap points will be awarded in Race 1 and Race 2. Teams that complete a lap would receive 2 points in Race 1 and 4 points in Race 2. Ranking points will be awarded in Race 1 and Race 2. The top five teams that have the fastest time will be awarded ranking points based on the table below. The ranking points of Race 2 are double of the ones from Race 1. The team score is the sum of the points accumulated in all the races. This final score will be compared against the final score of all the teams to determine the final ranking. The team with the most points wins. If multiple teams have the same number of points, they will receive the same ranking. Example Team 1 manages to assemble the car for Race 0 and run it on the track. In Race 1, the team runs the car but does not complete a lap, and in Race 2, the team finishes the lap and it is the fastest lap of all teams. As a result, Team 1 will receive 28 points: 2 points for running the car in Race 0, plus 2 points for running the car in Race 1, plus 4 points for finishing a lap in Race 2, and 20 points for ranking first in Race 2. Team 2 does not manage to assemble the car for Race 0. In Race 1 they finish a lap and rank third and in Race 2 the car runs on the track but does not complete a lap. As a result, team 2 will receive 10 points: 2 points for running the car in Race 1, plus 2 points for finishing a lap in Race 1, and 6 points for ranking third in Race 1. Awards \u00b6 Teams that competed before in Race On are allowed to switch to Jetson Nano once they earn 2 or more points. Teams that accumulate 4 points or more by the end of Race 1 will receive encoders and an inertial measurement unit as an award. At the end of the competition, after Race 2, teams will be awarded an Amazon gift card in the value of $10 for each accumulated point. Car Kit \u00b6 All teams compete with a car built around a 1/10 scale RC touring chassis. Distance between the front and back wheels, wheelbase, should be between 250 mm to 270 mm. Altering the contact surface of the tires is not allowed. Changing the tires to similar ones but with a different pattern is allowed. Cars should use one Mabuchi RS-540SH brushed motor to drive the car. Cars should use one battery, two LiPo cells with 7.4 V nominal and 8.4 V max, to power the car and the electronics. Converters that produce voltages above 8.4 V are not allowed. Cars should be controlled solely by a Raspberry Pi or Jetson Nano board and operate autonomously during the race. The camera must be the primary navigation sensor. Car dimensions should not exceed 12 inch width, 15 inch length, and 12 inch height with all sensors and additional boards mounted. Adding additional sensors such as accelerometer, gyroscope, encoders are allowed. Adjusting the suspension springs and weight balance, removing auxiliary chassis parts, and mounting additional boards is allowed. Additional modifications are allowed if they are easy to replicate by other teams. Attention To see whether your additional modifications are acceptable, contact Race On team before implementing them. If your modifications are illegal, you will not be allowed to compete in the races. Track \u00b6 The racing track is built out of black segments containing white lines for delimiting the track boundary. The dimensions of a track segment are 48 inch width and 48 inch length. The white lines delimiting the track are 2 inches wide and spaced 2 inches from the edge of the track segment. Track segments are of three types, Start/Finish Line, Straight Line, and 90\u00b0 Turn. The Start/Finish segment is similar to the Straight Line segment but has two white squares in the middle which are 2 inches wide and spaced 2 inches apart. Racing Day \u00b6 The racing day is composed of three parts, practice session in the morning followed by the car inspection around noon and ending with the race itself in the afternoon. The exact time of each part would be announced a week before the race day. Practice \u00b6 On the morning of the race day, each team will receive dedicate practice time on the race track. The start time and the duration of the practice session will be announced a week before. During the practice session, each time will receive 1 minute on the track and no other cars are allowed to be on the track during that time. At the end of the 1-minute practice run, teams are allowed to get back in line to request and additional minute. Inspection \u00b6 Before the race starts, all cars should be handed on for inspection. The judges will inspect the cars according to requirements specified in the cars section. Violation of the requirements would be penalized with extra time or disqualified based on the severity of the violation. Cars would stay in the inspection area until the beginning of the race. No mechanical modifications are allowed after this point without approval from the judges. Racing \u00b6 Only teams that passed the inspection are allowed to race. Teams will be grouped into 30 minutes slots. At the beginning of each slot, all the teams in that slot are required to come to the pre-race area to pick up their car. Teams are allocated into slots at random and find their racing order within the slot at the beginning of that slot. The team has 4 minutes in the pre-race area to set up their car and practice on a simple oval track. The team that is going to race next has priority on the practice track. After being called for racing, teams have 4 minutes to record a time. Teams have a practice run plus three attempts to finish the race and record the fastest time. The first run is considered a practice run and does not count for ranking. Teams, in order, will finish the first attempt before proceeding with the second and then the third. Cars are placed two to six feet before the finish line. During the race, cars should always be between the two white lines delimiting the track. If during the race one or more wheels completely cross the white line, the attempt is marked as failed.","title":"Rules"},{"location":"rules/#rules-spring-2020","text":"","title":"Rules \u2013 Spring 2020"},{"location":"rules/#registration","text":"Registration for the Spring 2020 edition is open until Sunday, January 19 . Teams that register must pay $250 materials fee if all members are new to Race On. Teams where one or more team members competed in a previous Race On competition still have to register online but do not pay the materials fee. We encourage teams to reuse previously received cars and we can help replace certain components. Teams are formed of two to four students that are currently enrolled at USC and are the only ones who build, program, and race the car. Each team delegates a contact person that will represent the team. All teams compete in the same races and on the same racing track irrespective of their program and year of study. Students are required to exhibit appropriate sportsmanship behavior and any attempts of cheating would result in penalties or disqualification.","title":"Registration"},{"location":"rules/#races","text":"Race On competition consists of three races with mandatory participation. Date Room Race 0 Saturday, February 8 EEB 132 Race 1 Saturday, February 29 EEB 132 Race 2 Friday, April 17 EEB 132 Teams are awarded points based on their performance in each race. In Race 0, teams will race on a simple loop track. The goal of this race is to stimulate teams to finish assembling their cars. In Race 1, teams will race on an average complexity track and will be awarded points for both participation and ranking. In Race 2 teams will race on a longer and more complex track. Teams will receive points for completing a lap and the ranking points will be double compared to the first race.","title":"Races"},{"location":"rules/#points","text":"Note The point system is designed such that a team that did average in Race 1 can still win the competition if they improve for Race 2. The best team from Race 1 does not automatically win if they do average in Race 2. However, winning Race 1 helps break some ties in the final standings. In all three races, teams will be awarded points for running the car, finishing a lap, and ranking in the top five in each race. Race 0 Race 1 Race 2 Run 2 2 - Lap - 2 4 Rank 1 - 10 20 Rank 2 - 8 16 Rank 3 - 6 12 Rank 4 - 4 8 Rank 5 - 2 4 Run points will be awarded in Race 0 and Race 1. Teams that manage to run their car, car moves autonomously on the track, will receive 2 points. Lap points will be awarded in Race 1 and Race 2. Teams that complete a lap would receive 2 points in Race 1 and 4 points in Race 2. Ranking points will be awarded in Race 1 and Race 2. The top five teams that have the fastest time will be awarded ranking points based on the table below. The ranking points of Race 2 are double of the ones from Race 1. The team score is the sum of the points accumulated in all the races. This final score will be compared against the final score of all the teams to determine the final ranking. The team with the most points wins. If multiple teams have the same number of points, they will receive the same ranking. Example Team 1 manages to assemble the car for Race 0 and run it on the track. In Race 1, the team runs the car but does not complete a lap, and in Race 2, the team finishes the lap and it is the fastest lap of all teams. As a result, Team 1 will receive 28 points: 2 points for running the car in Race 0, plus 2 points for running the car in Race 1, plus 4 points for finishing a lap in Race 2, and 20 points for ranking first in Race 2. Team 2 does not manage to assemble the car for Race 0. In Race 1 they finish a lap and rank third and in Race 2 the car runs on the track but does not complete a lap. As a result, team 2 will receive 10 points: 2 points for running the car in Race 1, plus 2 points for finishing a lap in Race 1, and 6 points for ranking third in Race 1.","title":"Points"},{"location":"rules/#awards","text":"Teams that competed before in Race On are allowed to switch to Jetson Nano once they earn 2 or more points. Teams that accumulate 4 points or more by the end of Race 1 will receive encoders and an inertial measurement unit as an award. At the end of the competition, after Race 2, teams will be awarded an Amazon gift card in the value of $10 for each accumulated point.","title":"Awards"},{"location":"rules/#car-kit","text":"All teams compete with a car built around a 1/10 scale RC touring chassis. Distance between the front and back wheels, wheelbase, should be between 250 mm to 270 mm. Altering the contact surface of the tires is not allowed. Changing the tires to similar ones but with a different pattern is allowed. Cars should use one Mabuchi RS-540SH brushed motor to drive the car. Cars should use one battery, two LiPo cells with 7.4 V nominal and 8.4 V max, to power the car and the electronics. Converters that produce voltages above 8.4 V are not allowed. Cars should be controlled solely by a Raspberry Pi or Jetson Nano board and operate autonomously during the race. The camera must be the primary navigation sensor. Car dimensions should not exceed 12 inch width, 15 inch length, and 12 inch height with all sensors and additional boards mounted. Adding additional sensors such as accelerometer, gyroscope, encoders are allowed. Adjusting the suspension springs and weight balance, removing auxiliary chassis parts, and mounting additional boards is allowed. Additional modifications are allowed if they are easy to replicate by other teams. Attention To see whether your additional modifications are acceptable, contact Race On team before implementing them. If your modifications are illegal, you will not be allowed to compete in the races.","title":"Car Kit"},{"location":"rules/#track","text":"The racing track is built out of black segments containing white lines for delimiting the track boundary. The dimensions of a track segment are 48 inch width and 48 inch length. The white lines delimiting the track are 2 inches wide and spaced 2 inches from the edge of the track segment. Track segments are of three types, Start/Finish Line, Straight Line, and 90\u00b0 Turn. The Start/Finish segment is similar to the Straight Line segment but has two white squares in the middle which are 2 inches wide and spaced 2 inches apart.","title":"Track"},{"location":"rules/#racing-day","text":"The racing day is composed of three parts, practice session in the morning followed by the car inspection around noon and ending with the race itself in the afternoon. The exact time of each part would be announced a week before the race day.","title":"Racing Day"},{"location":"rules/#practice","text":"On the morning of the race day, each team will receive dedicate practice time on the race track. The start time and the duration of the practice session will be announced a week before. During the practice session, each time will receive 1 minute on the track and no other cars are allowed to be on the track during that time. At the end of the 1-minute practice run, teams are allowed to get back in line to request and additional minute.","title":"Practice"},{"location":"rules/#inspection","text":"Before the race starts, all cars should be handed on for inspection. The judges will inspect the cars according to requirements specified in the cars section. Violation of the requirements would be penalized with extra time or disqualified based on the severity of the violation. Cars would stay in the inspection area until the beginning of the race. No mechanical modifications are allowed after this point without approval from the judges.","title":"Inspection"},{"location":"rules/#racing","text":"Only teams that passed the inspection are allowed to race. Teams will be grouped into 30 minutes slots. At the beginning of each slot, all the teams in that slot are required to come to the pre-race area to pick up their car. Teams are allocated into slots at random and find their racing order within the slot at the beginning of that slot. The team has 4 minutes in the pre-race area to set up their car and practice on a simple oval track. The team that is going to race next has priority on the practice track. After being called for racing, teams have 4 minutes to record a time. Teams have a practice run plus three attempts to finish the race and record the fastest time. The first run is considered a practice run and does not count for ranking. Teams, in order, will finish the first attempt before proceeding with the second and then the third. Cars are placed two to six feet before the finish line. During the race, cars should always be between the two white lines delimiting the track. If during the race one or more wheels completely cross the white line, the attempt is marked as failed.","title":"Racing"},{"location":"standings/spring-2020/","text":"Standings - Spring 2020 \u00b6 Overall Rankings \u00b6 Table with name, points race 0, points race 1, points race 2, total Name Race 0 points Race 1 points Race 2 points Total points Chien-Ming Chen 2 14 - 16 Yijia Zhang 2 10 - 12 Zhenghao Dai 0 12 - 12 Shiyuan Tian 2 8 - 10 Iris Liu 2 6 - 8 Avadhoot Ahire 2 4 - 6 Justin Ho 2 4 - 6 Shihong Ling 2 4 - 6 Chongxian Wang 2 4 - 6 Marshall Demirjian 1 4 - 5 Ranga Sai Shreyas Manchikanti 2 2 - 4 Hanyu She 2 2 - 4 Divyanhsu Sahay 0 4 - 4 Mason Mcinnis 0 4 - 4 Gabe Dalessandro 0 4 - 4 Pushpret Singh Hanspal 2 0 - 2 Usman Shahid 2 0 - 2 Alexander Vilesov 2 0 - 2 Zhiyuan Ning 0 1 - 1 Lingchen Lian 1 0 - 1 Tejas Harishchandra Acharya 0 0 - 0 Weiji Huang 0 0 - 0 Shatad Purohit 0 0 - 0 Jeff Szielenski 0 0 - 0 Yihang Chen 0 0 - 0 Atharva Fulay 0 0 - 0 Results for Race 0 \u00b6 Name Practice Run Run 1 Run 2 Run 3 Best time Points Yijia Zhang RUN 5.06 RUN 4.44 4.44 2 Pushpret Singh Hanspal 5.47 4.63 - - 4.63 2 Ranga Sai Shreyas Manchikanti 4.93 5.05 4.9 4.99 4.9 2 Shiyuan Tian 8 7.5 5.71 5.45 5.45 2 Avadhoot Ahire RUN 5.92 RUN RUN 5.92 2 Chien-Ming Chen 6.28 RUN 9.3 RUN 9.3 2 Iris Liu RUN RUN - - - 2 Justin Ho RUN RUN - - - 2 Hanyu She RUN RUN RUN RUN - 2 Usman Shahid RUN RUN RUN RUN - 2 Shihong Ling RUN RUN RUN RUN - 2 Alexander Vilesov RUN RUN - - - 2 Chongxian Wang RUN RUN RUN RUN - 2 Marshall Demirjian RUN RUN - - 1 Lingchen Lian RUN - - - - 1 Mason Mcinnis 9.47 8.78 RUN RUN 8.78 0 Gabe Dalessandro 12.84 12.67 11.03 11.03 0 Zhenghao Dai - - - - - 0 Tejas Harishchandra Acharya - - - - - 0 Divyanhsu Sahay - - - - - 0 Weiji Huang - - - - - 0 Shatad Purohit - - - - - 0 Jeff Szielenski - - - - - 0 Yihang Chen - - - - - 0 Zhiyuan Ning - - - - - 0 Atharva Fulay - - - - - 0 Penalized Results for Race 1 \u00b6 Name Practice Run Run 1 Run 2 Run 3 Best Time Points Chien-Ming Chen 9.904 9.398 8.787 8.509 8.509 14 Zhenghao Dai RUN 12.065 12.181 8.961 8.961 12 Yijia Zhang RUN RUN RUN 9.501 9.501 10 Shiyuan Tian RUN 9.616 RUN RUN 9.616 8 Iris Liu RUN 10.623 10.577 RUN 10.577 6 Gabe Dalessandro 12.6 14.039 11.459 RUN 11.459 4 Divyanhsu Sahay 12.677 RUN 12.879 RUN 12.879 4 Marshall Demirjian 13.942 14.267 RUN RUN 14.267 4 Shihong Ling RUN 16.796 RUN 15.982 15.982 4 Avadhoot Ahire RUN RUN 20.112 16.391 16.391 4 Justin Ho RUN 16.79 16.416 RUN 16.416 4 Chongxian Wang RUN 18.865 17.064 RUN 17.064 4 Mason Mcinnis 28.725 RUN 29.904 30.703 29.904 4 Ranga Sai Shreyas Manchikanti RUN RUN RUN RUN - 2 Hanyu She RUN RUN RUN RUN - 2 Zhiyuan Ning RUN RUN RUN RUN - 1 Pushpret Singh Hanspal - - - - - 0 Usman Shahid - - - - - 0 Alexander Vilesov - - - - - 0 Tejas Harishchandra Acharya - - - - - 0 Weiji Huang - - - - - 0 Shatad Purohit - - - - - 0 Jeff Szielenski - - - - - 0 Yihang Chen - - - - - 0 Atharva Fulay - - - - - 0 Lingchen Lian - - - - - 0 Penalized 1 point Results for Race 2 \u00b6 Race 2 is scheduled for April 18 in EEB 132.","title":"Standings - Spring 2020"},{"location":"standings/spring-2020/#standings-spring-2020","text":"","title":"Standings - Spring 2020"},{"location":"standings/spring-2020/#overall-rankings","text":"Table with name, points race 0, points race 1, points race 2, total Name Race 0 points Race 1 points Race 2 points Total points Chien-Ming Chen 2 14 - 16 Yijia Zhang 2 10 - 12 Zhenghao Dai 0 12 - 12 Shiyuan Tian 2 8 - 10 Iris Liu 2 6 - 8 Avadhoot Ahire 2 4 - 6 Justin Ho 2 4 - 6 Shihong Ling 2 4 - 6 Chongxian Wang 2 4 - 6 Marshall Demirjian 1 4 - 5 Ranga Sai Shreyas Manchikanti 2 2 - 4 Hanyu She 2 2 - 4 Divyanhsu Sahay 0 4 - 4 Mason Mcinnis 0 4 - 4 Gabe Dalessandro 0 4 - 4 Pushpret Singh Hanspal 2 0 - 2 Usman Shahid 2 0 - 2 Alexander Vilesov 2 0 - 2 Zhiyuan Ning 0 1 - 1 Lingchen Lian 1 0 - 1 Tejas Harishchandra Acharya 0 0 - 0 Weiji Huang 0 0 - 0 Shatad Purohit 0 0 - 0 Jeff Szielenski 0 0 - 0 Yihang Chen 0 0 - 0 Atharva Fulay 0 0 - 0","title":"Overall Rankings"},{"location":"standings/spring-2020/#results-for-race-0","text":"Name Practice Run Run 1 Run 2 Run 3 Best time Points Yijia Zhang RUN 5.06 RUN 4.44 4.44 2 Pushpret Singh Hanspal 5.47 4.63 - - 4.63 2 Ranga Sai Shreyas Manchikanti 4.93 5.05 4.9 4.99 4.9 2 Shiyuan Tian 8 7.5 5.71 5.45 5.45 2 Avadhoot Ahire RUN 5.92 RUN RUN 5.92 2 Chien-Ming Chen 6.28 RUN 9.3 RUN 9.3 2 Iris Liu RUN RUN - - - 2 Justin Ho RUN RUN - - - 2 Hanyu She RUN RUN RUN RUN - 2 Usman Shahid RUN RUN RUN RUN - 2 Shihong Ling RUN RUN RUN RUN - 2 Alexander Vilesov RUN RUN - - - 2 Chongxian Wang RUN RUN RUN RUN - 2 Marshall Demirjian RUN RUN - - 1 Lingchen Lian RUN - - - - 1 Mason Mcinnis 9.47 8.78 RUN RUN 8.78 0 Gabe Dalessandro 12.84 12.67 11.03 11.03 0 Zhenghao Dai - - - - - 0 Tejas Harishchandra Acharya - - - - - 0 Divyanhsu Sahay - - - - - 0 Weiji Huang - - - - - 0 Shatad Purohit - - - - - 0 Jeff Szielenski - - - - - 0 Yihang Chen - - - - - 0 Zhiyuan Ning - - - - - 0 Atharva Fulay - - - - - 0 Penalized","title":"Results for Race 0"},{"location":"standings/spring-2020/#results-for-race-1","text":"Name Practice Run Run 1 Run 2 Run 3 Best Time Points Chien-Ming Chen 9.904 9.398 8.787 8.509 8.509 14 Zhenghao Dai RUN 12.065 12.181 8.961 8.961 12 Yijia Zhang RUN RUN RUN 9.501 9.501 10 Shiyuan Tian RUN 9.616 RUN RUN 9.616 8 Iris Liu RUN 10.623 10.577 RUN 10.577 6 Gabe Dalessandro 12.6 14.039 11.459 RUN 11.459 4 Divyanhsu Sahay 12.677 RUN 12.879 RUN 12.879 4 Marshall Demirjian 13.942 14.267 RUN RUN 14.267 4 Shihong Ling RUN 16.796 RUN 15.982 15.982 4 Avadhoot Ahire RUN RUN 20.112 16.391 16.391 4 Justin Ho RUN 16.79 16.416 RUN 16.416 4 Chongxian Wang RUN 18.865 17.064 RUN 17.064 4 Mason Mcinnis 28.725 RUN 29.904 30.703 29.904 4 Ranga Sai Shreyas Manchikanti RUN RUN RUN RUN - 2 Hanyu She RUN RUN RUN RUN - 2 Zhiyuan Ning RUN RUN RUN RUN - 1 Pushpret Singh Hanspal - - - - - 0 Usman Shahid - - - - - 0 Alexander Vilesov - - - - - 0 Tejas Harishchandra Acharya - - - - - 0 Weiji Huang - - - - - 0 Shatad Purohit - - - - - 0 Jeff Szielenski - - - - - 0 Yihang Chen - - - - - 0 Atharva Fulay - - - - - 0 Lingchen Lian - - - - - 0 Penalized 1 point","title":"Results for Race 1"},{"location":"standings/spring-2020/#results-for-race-2","text":"Race 2 is scheduled for April 18 in EEB 132.","title":"Results for Race 2"},{"location":"workshops/1_ros/","text":"Introduction to ROS \u00b6 ROS is an open-source framework that aims to simplify the complexities involved in writing robust, general robotics software. ROS has a number of important filesystem level concepts, including packages, metapackages, package manifests, repositories, message types and service types. For Race On, however, we will focus mostly on the use of packages and message types . The power of ROS comes from the use of a computation graph similar to one used by Tensorflow, a machine learning library. The computation graph is composed from a number of nodes which either produce or consume data from multiple sources through a publish/subscribe architecture. At initialization, nodes declaretheir name and subscribe to the topics which provide all the necessary information for the node to properly function. Each topic has an associated message type which defines the format that the publishers and subscribers will use to exchange information. Some nodes might only publish data, such as the camera node, some might only subscribe to data topics, such as the motor and servo node, and some might do both such as the image processing node that receives images and outputs the car track position. ROS concepts review. Packages - In ROS, packages are the primary means of organizing software and are the smallest item you can build and release. They can contain any collection of files that should logically be contained together. Message Types - Message types describe message data structures and outline which data types they contain. When creating a message type, they should be contained in the `msg folder of your package. For more info about defining a custom message type in ROS, check out creating a ROS msg and srv . Nodes - Nodes are modular processes that perform some computation for your system. They should be a logical unit within your sytem. For example, for Race On, we are using 4 nodes (actuation.py, camera.py, control.py and pos_estimation.py), all of which handle a specific part of the car's behavior. Each of these nodes communicate by publishing and subscribing to messages on different topics . Topics - Nodes can send out and receive messages by publishing and subscribing to topics . Each topic has a name, which should be associated with the message types or message content being sent through it. For example, the control node in the race-on-ros repository subscribes to Pose messages on the position/error topic and publishes AckermanDrive messages on the control topic. Note that you can only send one message type through a single topic. Messages - Messages are simple data structures that can contain a combination of basic data types. For a more in depth discussion of common ROS concepts, check out ROS concepts . Preparation \u00b6 Before we proceed with this quickstart quide we need to perform a few preparation steps. If you plan to perform these steps using a Jupyter terminal instead of ssh, please before starting close all Jupyter tabs except the terminal. Update the list of packages and install all the available software updates. 1 2 sudo apt update sudo apt upgrade Install the OpenCV library for Python3. We will use the OpenCV functions for camera calibration and image preprocessing. 1 sudo apt install python3-opencv Instruct ROS to use Python 3 by setting the ROS_PYTHON_VERSION environment variable. 1 echo \"export ROS_PYTHON_VERSION=3\" >> ~/.bashrc The .bashrc file in the home folder contains all the commands which are executed when you open a terminal. The startup folder in the home directory contains the scripts which are run everytime the Pi boots to setup the hardware and start the Jupyter and the ROS services. We need to modify the ROS script to allow the external devices connect to the ROS running on the Pi. For that open the file ~/startup/ros.sh using the Jupyter interface and, if present, remove the line export ROS_HOSTNAME=raspberrypi\" . In addition, since these scripts are crucial for the Race On platform to function properly we will make them read only to prevent accedental edits. For that run the following command in the terminal. 1 chmod a-w ~/startup/* where the argument a stands for all, - remove, and w write permission. Now we can prepare the ROS workspace. The race-on-ros folder contains the GitHub repository of the Race On team and only Race On organizers can push updates back to GitHub. To solve this issue we need to first delete the folder, fork the GitHub repository using your own GitHub account where you can add collaborators, and then clone back on Pi the forked repository. To delete the folder, run 1 rm -rf ~/race-on-ros Next, go to the GitHub page of the race-on-ros repository and click the fork button on the upper left corner. After a few seconds you should be redirected to your own race-on-ros repository. Clone the forked repository using the following command 1 git clone https://github.com/<your-username>/race-on-ros.git Now you should see again the race-on-ros folder in your home directory but this time the repository is linked to your own GitHub account. Before we can use the raceon package we need to build it since only source files are commited to GitHub. The following two commands are crucial and if you later experience problems with the ROS code then most likely you forgot to run them. You need to run these commands every time you add new files or dependencies to your packages so ROS knows about them. 1 2 catkin_make -C ~/race-on-ros/ source ~/race-on-ros/devel/setup.bash First line compiles all the packages in the workspace whereas the second command lets ROS know about the new compiled files. Moreover, the catkin_make command should also create two additional folders in the workspace, build and devel . The build folder is the default location of the build space and is where cmake and make are called to configure and build your packages. The devel folder is the default location of the devel space, which is where your executables and libraries go before you install your packages. Congratulations, you successfully completed all the steps required to setup the Race On ROS environment. But before proceeding with the next section reboot to apply the updates. 1 sudo reboot ROS Tutorial \u00b6 Before we introduce the Race On code for ROS we will do a quick tutorial. In ROS, code is organized around packages which are folders with special structure inside the src folder of the workspace. If you run ls ~/race-on-ros/src you will see that the race-on-ros workspace contains only the raceon package. In this tutorial we will create a new package called tutorial that will have two nodes, the publisher node that publishes a sequence of numbers to the data topic and the subscriber node that will subscribe to the data topic to receive the sequence. If you're looking for more, here's another tutorial which provides information on creating a package in a catkin workspace . To create a new package we have first to make ~/race-on-ros/src the current directory and then run the ROS catkin_create_pkg to create a package folder. 1 2 cd ~/race-on-ros/src catkin_create_pkg tutorial std_msgs rospy The arguments for the catkin_create_pkg command are the package name followed by the dependencies of the new package. In this case, the dependencies are std_msgs and rospy . Since we will send Float32 numbers we will use std_msgs as it includes common message types representing primitive data types and other basic message constructs. And rospy since we will write our code in Python. If you plan to write code in C or C++ you must also add roscpp to the dependency list. In case you forget to add a package as dependence, you can do that later by editing one of the package configuration files. To check the result of the command we can list the contents of the tutorial folder using ls -ahl tutorial/ and you should see an output similar to this: 1 2 3 4 5 6 total 24K drwxr-xr-x 3 pi pi 4 .0K Feb 15 03 :06 . drwxr-xr-x 4 pi pi 4 .0K Feb 15 03 :06 .. -rw-r--r-- 1 pi pi 6 .9K Feb 15 03 :06 CMakeLists.txt -rw-r--r-- 1 pi pi 2 .7K Feb 15 03 :06 package.xml drwxr-xr-x 2 pi pi 4 .0K Feb 15 03 :06 src Where package.xml contains the package configuration in XML format, CMakeLists.txt contains the instructions how to build the C++ code, and the src is the directory where you place all your source files. To change the dependency list of your package at a later time or customize the package, edit the package.xml file. However, since our code will be only written in Python, we will create a separate folder called scripts to place our Python programs using the mkdir tutorial/scripts command. To build the package run 1 2 cd ~/race-on-ros/ catkin_make Since we did not write any code this will should not encounter any errors, just see that catkin , the build tool of ROS, recognized the new package and traversed it contents. Create the publisher node. In ROS, a \"node\" is an executable that is connected to the ROS network. Here we will create the publisher node which will continuously send samples generated using a sinusoidal signal. Using the Jupyter interface, navigate to the race-on-ros/src/tutorial/scripts/ folder and create a new text file called publisher.py . Paste this content inside the file and save it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/usr/bin/env python import rospy from std_msgs.msg import Float32 import math RATE = 10 # Publishing rate of new data per second FREQ = 1 # Frequency of the sinusoidal signal # Execute this when run as a script if __name__ == '__main__' : rospy . init_node ( 'publisher' ) pub = rospy . Publisher ( 'data' , Float32 , queue_size = 1 ) rate = rospy . Rate ( RATE ) step = 0 while not rospy . is_shutdown (): # Generate new data point value = math . sin ( 2 * math . pi * step / RATE ) # Log and publish data rospy . loginfo ( \"Publishing {:.3f} \" . format ( value )) pub . publish ( value ) # Advance the sequence step = step + 1 # Wait to match the rate rate . sleep () The break down. 1 #!/usr/bin/env python This tells the terminal that this is a Python script. All Python files should have it. 1 2 3 4 import rospy from std_msgs.msg import Float32 import math We import the Python libraries which we declared as dependencies for the tutorial package and the math library required for the to calculate the sinusoid values. The python rospy library provides all the ROS functionality to Python whereas python std_msgs library the standard message types in ROS. 1 2 RATE = 10 # Publishing rate of new data per second FREQ = 1 # Frequency of the sinusoidal signal Next, we declare two global constants which are parameters of the generated sequence. The code inside the if statement represents the main program. Here we define how the publisher node interfaces with the rest of ROS. The line rospy.init_node(NAME) , is very important as it tells rospy the name of the node -- until rospy has this information, it cannot start communicating with the ROS Master . In this case, the node will take on the name publisher . NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Publisher('data', Float32, queue_size=1) declares that your node is publishing to the data topic using the message type Float32 . Float32 here is actually the class std_msgs.msg.Float32. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. rate = rospy.Rate(RATE) this line creates a Rate object rate. With the help of its method sleep(), it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10 th of a second!). Next we have a fairly standard rospy loop construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). In this case, the \"work\" is a call to pub.publish(value) that publishes a string to our data topic. The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop. This loop also calls rospy.loginfo(str) , which performs triple-duty: the messages get printed to screen, it gets written to the Node's log file, and it gets written to rosout . rosout is a handy tool for debugging: you can pull up messages using rqt_console instead of having to find the console window with your Node's output. To run the code we need first to make the script executable by changing the file permission using chmod +x ~/race-on-ros/src/tutorial/scripts/publisher.py command. Now, we can run the code using rosrun tutorial publisher.py command. Note, tab autocompletion works with ROS command arguments. The code for the subscriber is similar and the steps are the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/usr/bin/env python import rospy from std_msgs.msg import Float32 # Automatically called by ROS when new message is received def callback ( message ): rospy . loginfo ( \"Received {:.3f} \" . format ( message . data )) # Execute this when run as a script if __name__ == '__main__' : # Init the node and subscribe for data rospy . init_node ( \"subscriber\" ) rospy . Subscriber ( \"data\" , Float32 , callback ) # Prevents python from exiting until this node is stopped rospy . spin () Now you can open a second terminal to run the subscriber. Remember to make the subscriber script executable. Exerices To better understand ROS concepts try to improve the above code by adding the following features: Create a launch file to start both nodes at the same time. Consult ROS documentation and raceon package code for inspiration. Replace the global constants in the publisher.py file with ROS parameters. Update the launch file to include the default parameter values. Find how to overwrite the parameter value when you launch the application. Create a custom message type that contains two float number. Set the value of the second float to be the cosine. Use rqt ROS tool to plot in real-time these values on your machine. Overview of Race On ROS Code \u00b6 The ROS repository for Race On consists of 4 ROS nodes and one Python class that initializes the car configuration (including the servo's middle and rightmost values and the motor's min, max and brake speeds) and provides functions for steering and changing the speed of the car. The 4 nodes included in the repository are actuation.py, camera.py, control.py and pos_estimation.py. The nodes should be run using the file race-on-ros/src/launch/raceon.launch . Read more below to learn more about each of the nodes and the launch file we've provided to run them. Launch File \u00b6 The launch file has a number of parameters for each of the nodes, which you can customize according to your needs. It also defines the topics to which the nodes publish and subscribe. For example, the camera node publishes to either camera/image or camera/image/compressed topic, depending on the value of the ~publish_raw parameter, which is defined below on line 13 of the launch file. Exercise \u00b6 Read through the launch file to get a better understanding of how it works. Change or remove the parameters defined in it to see how the nodes respond. Camera Node \u00b6 Topics Message Type Action camera/image Image Publish camera/image/compressed CompressedImage Publish The camera node, which is found in race-on-ros/src/scripts/camera.py starts up the pi's camera and begins recording upon startup. Depending on the value of the ~use_compressed_image parameter, it will either publish compressed images on the camera/image/compressed or uncompressed images on the camera/image topic. The resolution , frames per second and publish_raw parameters can be configured in the launch file. Suggested Exercise \u00b6 Try adjusting the frame rate from the launch file and see how it affects your car's trajectory. Suggested Exercise \u00b6 Try adjusting the frame rate from the launch file and see how it affects your car's trajectory! Position Estimation Node \u00b6 Topics Message Type Action camera/image Image Subscribe camera/image/compressed CompressedImage Subscribe position/error Pose Publish `position/track TrackPosition Publish The position estimation node is found in race-on-ros/src/scripts/pos_estimation.py . This node executes much of the position-estimation functionality you likely saw before in the testcar-nomiddleline jupyter notebook . Depending on the value of the use_compressed_image parameter, the camera node will either subscribe to the camera/image/compressed or camera/image topic. You can set the scan line , peak threshold , track width and camera center parameters in the launch file or adjust their default values in the node's code. The scan line parameter is the horizontal line in the image that you'll use to look for peaks. When the node receives an image message, it finds peaks along the horizontal scan line and uses them to estimate the car's position within the track. It then publishes this position estimation to the \"position/error\" topic . It also publishes an estimation of the track position to the \"position/track\" topic . Suggested Exercise \u00b6 Try changing the peak threshold and see how it affects your estimation of the track and car positions! Controller Node \u00b6 Topics Message Type Action position/error Pose Subscribe control AckermanDrive Publish The controller node is found in race-on-ros/src/scripts/controller.py . The control node is where you'll want to put your team's control logic (potentially a PID controller?). Currently, the control node uses a proportional gain coefficient to determine how much you should alter the servo's position to minimize the distance from the target position. You can change the motor speed , target (target x position on the track) and kp (proportional gain coefficient to use to scale the error) parameters in the launch file or alter their default values in the code. When the controller node receives a Pose message containing the car's current position on the track, it calculates the error from the target position and estimates a new servo position value using kp to steer the car closer to the target. It then publishes this new servo position and the current motor speed to the control topic . Suggested Exercises \u00b6 Adjust the motor speed and proportional gain parameters and see how they alter your car's behavior. Actuator Node \u00b6 Topics Message Type Action control AckermanDrive Subscribe The actuator node is found in race-on-ros/src/scripts/actuation.py . It subscribes to the control topic and is responsible for commanding the car . To do so, it makes use of the car module, which has utility functions to set the car's speed and servo position. When the actuator node receives a control message, it uses the car module functions to steer the car and set its speed. Car Module \u00b6 The car module is a class that initializes the servo and motor and provides functions to alter the car's speed and the servo's position and also to brake the car. Setting Up ROS for Your Car \u00b6 To run the ros nodes, run roslaunch raceon raceon.launch speed:=140 .","title":"1. Introduction to ROS"},{"location":"workshops/1_ros/#introduction-to-ros","text":"ROS is an open-source framework that aims to simplify the complexities involved in writing robust, general robotics software. ROS has a number of important filesystem level concepts, including packages, metapackages, package manifests, repositories, message types and service types. For Race On, however, we will focus mostly on the use of packages and message types . The power of ROS comes from the use of a computation graph similar to one used by Tensorflow, a machine learning library. The computation graph is composed from a number of nodes which either produce or consume data from multiple sources through a publish/subscribe architecture. At initialization, nodes declaretheir name and subscribe to the topics which provide all the necessary information for the node to properly function. Each topic has an associated message type which defines the format that the publishers and subscribers will use to exchange information. Some nodes might only publish data, such as the camera node, some might only subscribe to data topics, such as the motor and servo node, and some might do both such as the image processing node that receives images and outputs the car track position. ROS concepts review. Packages - In ROS, packages are the primary means of organizing software and are the smallest item you can build and release. They can contain any collection of files that should logically be contained together. Message Types - Message types describe message data structures and outline which data types they contain. When creating a message type, they should be contained in the `msg folder of your package. For more info about defining a custom message type in ROS, check out creating a ROS msg and srv . Nodes - Nodes are modular processes that perform some computation for your system. They should be a logical unit within your sytem. For example, for Race On, we are using 4 nodes (actuation.py, camera.py, control.py and pos_estimation.py), all of which handle a specific part of the car's behavior. Each of these nodes communicate by publishing and subscribing to messages on different topics . Topics - Nodes can send out and receive messages by publishing and subscribing to topics . Each topic has a name, which should be associated with the message types or message content being sent through it. For example, the control node in the race-on-ros repository subscribes to Pose messages on the position/error topic and publishes AckermanDrive messages on the control topic. Note that you can only send one message type through a single topic. Messages - Messages are simple data structures that can contain a combination of basic data types. For a more in depth discussion of common ROS concepts, check out ROS concepts .","title":"Introduction to ROS"},{"location":"workshops/1_ros/#preparation","text":"Before we proceed with this quickstart quide we need to perform a few preparation steps. If you plan to perform these steps using a Jupyter terminal instead of ssh, please before starting close all Jupyter tabs except the terminal. Update the list of packages and install all the available software updates. 1 2 sudo apt update sudo apt upgrade Install the OpenCV library for Python3. We will use the OpenCV functions for camera calibration and image preprocessing. 1 sudo apt install python3-opencv Instruct ROS to use Python 3 by setting the ROS_PYTHON_VERSION environment variable. 1 echo \"export ROS_PYTHON_VERSION=3\" >> ~/.bashrc The .bashrc file in the home folder contains all the commands which are executed when you open a terminal. The startup folder in the home directory contains the scripts which are run everytime the Pi boots to setup the hardware and start the Jupyter and the ROS services. We need to modify the ROS script to allow the external devices connect to the ROS running on the Pi. For that open the file ~/startup/ros.sh using the Jupyter interface and, if present, remove the line export ROS_HOSTNAME=raspberrypi\" . In addition, since these scripts are crucial for the Race On platform to function properly we will make them read only to prevent accedental edits. For that run the following command in the terminal. 1 chmod a-w ~/startup/* where the argument a stands for all, - remove, and w write permission. Now we can prepare the ROS workspace. The race-on-ros folder contains the GitHub repository of the Race On team and only Race On organizers can push updates back to GitHub. To solve this issue we need to first delete the folder, fork the GitHub repository using your own GitHub account where you can add collaborators, and then clone back on Pi the forked repository. To delete the folder, run 1 rm -rf ~/race-on-ros Next, go to the GitHub page of the race-on-ros repository and click the fork button on the upper left corner. After a few seconds you should be redirected to your own race-on-ros repository. Clone the forked repository using the following command 1 git clone https://github.com/<your-username>/race-on-ros.git Now you should see again the race-on-ros folder in your home directory but this time the repository is linked to your own GitHub account. Before we can use the raceon package we need to build it since only source files are commited to GitHub. The following two commands are crucial and if you later experience problems with the ROS code then most likely you forgot to run them. You need to run these commands every time you add new files or dependencies to your packages so ROS knows about them. 1 2 catkin_make -C ~/race-on-ros/ source ~/race-on-ros/devel/setup.bash First line compiles all the packages in the workspace whereas the second command lets ROS know about the new compiled files. Moreover, the catkin_make command should also create two additional folders in the workspace, build and devel . The build folder is the default location of the build space and is where cmake and make are called to configure and build your packages. The devel folder is the default location of the devel space, which is where your executables and libraries go before you install your packages. Congratulations, you successfully completed all the steps required to setup the Race On ROS environment. But before proceeding with the next section reboot to apply the updates. 1 sudo reboot","title":"Preparation"},{"location":"workshops/1_ros/#ros-tutorial","text":"Before we introduce the Race On code for ROS we will do a quick tutorial. In ROS, code is organized around packages which are folders with special structure inside the src folder of the workspace. If you run ls ~/race-on-ros/src you will see that the race-on-ros workspace contains only the raceon package. In this tutorial we will create a new package called tutorial that will have two nodes, the publisher node that publishes a sequence of numbers to the data topic and the subscriber node that will subscribe to the data topic to receive the sequence. If you're looking for more, here's another tutorial which provides information on creating a package in a catkin workspace . To create a new package we have first to make ~/race-on-ros/src the current directory and then run the ROS catkin_create_pkg to create a package folder. 1 2 cd ~/race-on-ros/src catkin_create_pkg tutorial std_msgs rospy The arguments for the catkin_create_pkg command are the package name followed by the dependencies of the new package. In this case, the dependencies are std_msgs and rospy . Since we will send Float32 numbers we will use std_msgs as it includes common message types representing primitive data types and other basic message constructs. And rospy since we will write our code in Python. If you plan to write code in C or C++ you must also add roscpp to the dependency list. In case you forget to add a package as dependence, you can do that later by editing one of the package configuration files. To check the result of the command we can list the contents of the tutorial folder using ls -ahl tutorial/ and you should see an output similar to this: 1 2 3 4 5 6 total 24K drwxr-xr-x 3 pi pi 4 .0K Feb 15 03 :06 . drwxr-xr-x 4 pi pi 4 .0K Feb 15 03 :06 .. -rw-r--r-- 1 pi pi 6 .9K Feb 15 03 :06 CMakeLists.txt -rw-r--r-- 1 pi pi 2 .7K Feb 15 03 :06 package.xml drwxr-xr-x 2 pi pi 4 .0K Feb 15 03 :06 src Where package.xml contains the package configuration in XML format, CMakeLists.txt contains the instructions how to build the C++ code, and the src is the directory where you place all your source files. To change the dependency list of your package at a later time or customize the package, edit the package.xml file. However, since our code will be only written in Python, we will create a separate folder called scripts to place our Python programs using the mkdir tutorial/scripts command. To build the package run 1 2 cd ~/race-on-ros/ catkin_make Since we did not write any code this will should not encounter any errors, just see that catkin , the build tool of ROS, recognized the new package and traversed it contents. Create the publisher node. In ROS, a \"node\" is an executable that is connected to the ROS network. Here we will create the publisher node which will continuously send samples generated using a sinusoidal signal. Using the Jupyter interface, navigate to the race-on-ros/src/tutorial/scripts/ folder and create a new text file called publisher.py . Paste this content inside the file and save it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/usr/bin/env python import rospy from std_msgs.msg import Float32 import math RATE = 10 # Publishing rate of new data per second FREQ = 1 # Frequency of the sinusoidal signal # Execute this when run as a script if __name__ == '__main__' : rospy . init_node ( 'publisher' ) pub = rospy . Publisher ( 'data' , Float32 , queue_size = 1 ) rate = rospy . Rate ( RATE ) step = 0 while not rospy . is_shutdown (): # Generate new data point value = math . sin ( 2 * math . pi * step / RATE ) # Log and publish data rospy . loginfo ( \"Publishing {:.3f} \" . format ( value )) pub . publish ( value ) # Advance the sequence step = step + 1 # Wait to match the rate rate . sleep () The break down. 1 #!/usr/bin/env python This tells the terminal that this is a Python script. All Python files should have it. 1 2 3 4 import rospy from std_msgs.msg import Float32 import math We import the Python libraries which we declared as dependencies for the tutorial package and the math library required for the to calculate the sinusoid values. The python rospy library provides all the ROS functionality to Python whereas python std_msgs library the standard message types in ROS. 1 2 RATE = 10 # Publishing rate of new data per second FREQ = 1 # Frequency of the sinusoidal signal Next, we declare two global constants which are parameters of the generated sequence. The code inside the if statement represents the main program. Here we define how the publisher node interfaces with the rest of ROS. The line rospy.init_node(NAME) , is very important as it tells rospy the name of the node -- until rospy has this information, it cannot start communicating with the ROS Master . In this case, the node will take on the name publisher . NOTE: the name must be a base name, i.e. it cannot contain any slashes \"/\". The rospy.Publisher('data', Float32, queue_size=1) declares that your node is publishing to the data topic using the message type Float32 . Float32 here is actually the class std_msgs.msg.Float32. The queue_size argument limits the amount of queued messages if any subscriber is not receiving them fast enough. rate = rospy.Rate(RATE) this line creates a Rate object rate. With the help of its method sleep(), it offers a convenient way for looping at the desired rate. With its argument of 10, we should expect to go through the loop 10 times per second (as long as our processing time does not exceed 1/10 th of a second!). Next we have a fairly standard rospy loop construct: checking the rospy.is_shutdown() flag and then doing work. You have to check is_shutdown() to check if your program should exit (e.g. if there is a Ctrl-C or otherwise). In this case, the \"work\" is a call to pub.publish(value) that publishes a string to our data topic. The loop calls rate.sleep() , which sleeps just long enough to maintain the desired rate through the loop. This loop also calls rospy.loginfo(str) , which performs triple-duty: the messages get printed to screen, it gets written to the Node's log file, and it gets written to rosout . rosout is a handy tool for debugging: you can pull up messages using rqt_console instead of having to find the console window with your Node's output. To run the code we need first to make the script executable by changing the file permission using chmod +x ~/race-on-ros/src/tutorial/scripts/publisher.py command. Now, we can run the code using rosrun tutorial publisher.py command. Note, tab autocompletion works with ROS command arguments. The code for the subscriber is similar and the steps are the same. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/usr/bin/env python import rospy from std_msgs.msg import Float32 # Automatically called by ROS when new message is received def callback ( message ): rospy . loginfo ( \"Received {:.3f} \" . format ( message . data )) # Execute this when run as a script if __name__ == '__main__' : # Init the node and subscribe for data rospy . init_node ( \"subscriber\" ) rospy . Subscriber ( \"data\" , Float32 , callback ) # Prevents python from exiting until this node is stopped rospy . spin () Now you can open a second terminal to run the subscriber. Remember to make the subscriber script executable. Exerices To better understand ROS concepts try to improve the above code by adding the following features: Create a launch file to start both nodes at the same time. Consult ROS documentation and raceon package code for inspiration. Replace the global constants in the publisher.py file with ROS parameters. Update the launch file to include the default parameter values. Find how to overwrite the parameter value when you launch the application. Create a custom message type that contains two float number. Set the value of the second float to be the cosine. Use rqt ROS tool to plot in real-time these values on your machine.","title":"ROS Tutorial"},{"location":"workshops/1_ros/#overview-of-race-on-ros-code","text":"The ROS repository for Race On consists of 4 ROS nodes and one Python class that initializes the car configuration (including the servo's middle and rightmost values and the motor's min, max and brake speeds) and provides functions for steering and changing the speed of the car. The 4 nodes included in the repository are actuation.py, camera.py, control.py and pos_estimation.py. The nodes should be run using the file race-on-ros/src/launch/raceon.launch . Read more below to learn more about each of the nodes and the launch file we've provided to run them.","title":"Overview of Race On ROS Code"},{"location":"workshops/1_ros/#launch-file","text":"The launch file has a number of parameters for each of the nodes, which you can customize according to your needs. It also defines the topics to which the nodes publish and subscribe. For example, the camera node publishes to either camera/image or camera/image/compressed topic, depending on the value of the ~publish_raw parameter, which is defined below on line 13 of the launch file.","title":"Launch File"},{"location":"workshops/1_ros/#exercise","text":"Read through the launch file to get a better understanding of how it works. Change or remove the parameters defined in it to see how the nodes respond.","title":"Exercise"},{"location":"workshops/1_ros/#camera-node","text":"Topics Message Type Action camera/image Image Publish camera/image/compressed CompressedImage Publish The camera node, which is found in race-on-ros/src/scripts/camera.py starts up the pi's camera and begins recording upon startup. Depending on the value of the ~use_compressed_image parameter, it will either publish compressed images on the camera/image/compressed or uncompressed images on the camera/image topic. The resolution , frames per second and publish_raw parameters can be configured in the launch file.","title":"Camera Node"},{"location":"workshops/1_ros/#suggested-exercise","text":"Try adjusting the frame rate from the launch file and see how it affects your car's trajectory.","title":"Suggested Exercise"},{"location":"workshops/1_ros/#suggested-exercise_1","text":"Try adjusting the frame rate from the launch file and see how it affects your car's trajectory!","title":"Suggested Exercise"},{"location":"workshops/1_ros/#position-estimation-node","text":"Topics Message Type Action camera/image Image Subscribe camera/image/compressed CompressedImage Subscribe position/error Pose Publish `position/track TrackPosition Publish The position estimation node is found in race-on-ros/src/scripts/pos_estimation.py . This node executes much of the position-estimation functionality you likely saw before in the testcar-nomiddleline jupyter notebook . Depending on the value of the use_compressed_image parameter, the camera node will either subscribe to the camera/image/compressed or camera/image topic. You can set the scan line , peak threshold , track width and camera center parameters in the launch file or adjust their default values in the node's code. The scan line parameter is the horizontal line in the image that you'll use to look for peaks. When the node receives an image message, it finds peaks along the horizontal scan line and uses them to estimate the car's position within the track. It then publishes this position estimation to the \"position/error\" topic . It also publishes an estimation of the track position to the \"position/track\" topic .","title":"Position Estimation Node"},{"location":"workshops/1_ros/#suggested-exercise_2","text":"Try changing the peak threshold and see how it affects your estimation of the track and car positions!","title":"Suggested Exercise"},{"location":"workshops/1_ros/#controller-node","text":"Topics Message Type Action position/error Pose Subscribe control AckermanDrive Publish The controller node is found in race-on-ros/src/scripts/controller.py . The control node is where you'll want to put your team's control logic (potentially a PID controller?). Currently, the control node uses a proportional gain coefficient to determine how much you should alter the servo's position to minimize the distance from the target position. You can change the motor speed , target (target x position on the track) and kp (proportional gain coefficient to use to scale the error) parameters in the launch file or alter their default values in the code. When the controller node receives a Pose message containing the car's current position on the track, it calculates the error from the target position and estimates a new servo position value using kp to steer the car closer to the target. It then publishes this new servo position and the current motor speed to the control topic .","title":"Controller Node"},{"location":"workshops/1_ros/#suggested-exercises","text":"Adjust the motor speed and proportional gain parameters and see how they alter your car's behavior.","title":"Suggested Exercises"},{"location":"workshops/1_ros/#actuator-node","text":"Topics Message Type Action control AckermanDrive Subscribe The actuator node is found in race-on-ros/src/scripts/actuation.py . It subscribes to the control topic and is responsible for commanding the car . To do so, it makes use of the car module, which has utility functions to set the car's speed and servo position. When the actuator node receives a control message, it uses the car module functions to steer the car and set its speed.","title":"Actuator Node"},{"location":"workshops/1_ros/#car-module","text":"The car module is a class that initializes the servo and motor and provides functions to alter the car's speed and the servo's position and also to brake the car.","title":"Car Module"},{"location":"workshops/1_ros/#setting-up-ros-for-your-car","text":"To run the ros nodes, run roslaunch raceon raceon.launch speed:=140 .","title":"Setting Up ROS for Your Car"},{"location":"workshops/2_gain_scheduling/","text":"Gain Scheduling \u00b6 You assembled your car, connected it to your computer, tested the electronics, and hopefully got to run a few laps. So, what's next? Yoda has the answer for you! We have already encouraged you to improve the original self-driving code by adding integral and derivative terms to the PID controller. We will now introduce a new control technique called Gain Scheduling , which can be seen as an extension of PID. Note The following is a very simplified view of the topics. This is just to provide you an intuition of how the controllers work. Feedback Control \u00b6 Let's start with a quick recap on feedback control. One of the simplest ways to represent a typical control problem is through a block diagram. Our goal is to make the system output y as close as possible to the reference r . In order to do that, we use a sensor to measure variable y , compute the difference between current and desired value to obtain the error e and feed it to a controller. And here is where the magic happens. The controller block is responsible for transforming the measured error e into a command u which brings the system's output y closer to the reference r . Exercise : Try to identify each of the elements in the block diagram in the context of Race On. What is the system and what is the sensor? Which variables in your code are r , e , y and u ? Controllers can take very different forms. One example is the on-off mechanism of the heaters with a thermostat: the heater turns on when the temperature is below the reference and it turns off when the temperature is above the reference. Another example is the proportional controller, which can be described by u = Ke . We can also have a controller which computes system inputs based not only on the current error, but also based on the error integral and derivative. This leads to the famous PID controller . Note, the above diagram is for continuous time systems, our car is a discrete time system since we send one control command per image and not continuously while waiting for the next image. In discrete time systems the integral is replaced with the sum operator and the derivative with the difference between current and past error. The controller gains Kp , Ki , and Kd have to be selected such that the system presents the desired behavior (fast convergence to r , low overshooting etc). If we have a mathematical model of the system, there are analytical and graphical tools that help tuning the gains. In other cases, the gains are set using trial and error. Exercises : If you haven't yet, try to implement a PID controller for your car. You can reference this pseudocode . Search online for hints on how to tune the different gains. What are the expected effects of increasing/decreasing each one of them? Step Response \u00b6 Before we dive deeper into more advanced controllers, we first need to know how to measure the performance of different controllers. The most simple and at the same time informative method is the step response shown in the figure below. The step response is obtained by suddenly cheanging the reference from one value to another, in the figure above is from zero to one, and tracking how the controller responds to this change. To get a step response using you car, run the car on a straight line and after one second switch the reference value from CAMERA_CENTER to CAMERA_CENTER - 100 and save the values of line_pos into an array, stop the car after a timeout and plot the array to see the step response. Repeat with a step in a different direction CAMERA_CENTER + 100 to make sure your car responds to left and right turns equally. The most important metrics of the step response are rise time , overshoot , and settling time .We want a fast rise time with no overshoot and low settling time, however, in practice we have to accept a tradeoff as optimizing all three at the same time is most of the time impossible. Rise time is defined as the time from when the reference value changed until we reach the new reference (or 0.9 of the new refence value). A high gain (high value of the proportional and derivative coefficients) leads to a fast response time but also to an overshoot. Overshoot defines by how much we miss the new reference once we reach the new reference value for the first time. A high overshoot can make you car run off the track as you exit the turn, but a small overshoot can be tolerated and help decreases the rise time. Settling time is the time between the reference change and the time the system stabilizes around it. A low settling time helps if there are a sequence of turns as the car will stay closer to the reference. Gain Scheduling \u00b6 A well tuned PID controller can yield very good performance. However, what happens if the operating condition changes? Airplanes are a good example. The system behaves remarkably different at take-off, in-flight and on landing. In these cases, the gains tuned for one operating condition might not lead to the desired behavior when the system is at another operating point. One possible solution to this problem is gain scheduling. It is usually done in 4 steps: Define the different system operating conditions; Tune your controller gains for each operating condition. This will create an array (or a table) of controller gains; Determine how to vary the controller gains based on the operating conditions; Assess system performance. Steps 1, 2, and 4 are the same as you would do for a regular PID controller. Step 3 is the fun part! The main options are: Simple switch : change the gains when the system is at a different operating point. The advantage of this method is its simplicity. However, this creates discontinuities. If the error is 2 and the gain changes from 2 to 5, your control input jumps from 4 to 10! This can damage actuators or create undesirable system behavior. Transient switch : when the operation conditions change, linearly change from one gain to the other within a certain time interval. The trick here is to choose the time interval long enough to create a smooth transient but short enough to allow the controller to respond properly to the operating condition change. Interpolate between gains : we can avoid switching by creating a curve that smoothly changes between gains continuously. This is possible when the operating conditions also change continuously. The three cases are illustrated below, where tc is the time when the operating condition change is identified, and T is the transient time for the transient switch approach. For a more in depth introduction to gain scheduling, we recommend watching this video . Applied to Race On \u00b6 For now, let's assume your car is set to a constant speed and we only want to control the position relative to the center of the track. We will reference the original code provided with the car but it should be easy to adapt these instructions to your modified code. If you have any issues with it, remember you can always check the original code at the race on github. We follow the 4 steps described previously: Define the different system operating conditions. Even though the car itself doesn't change much during the race, the track varies considerably. We can define three different operating modes for the car: straight line, 90 o turn and 180 o turn. Next, we have to find a way to identify these conditions. One way of doing this is by looking at the error. This requires a few steps and some tuning. Let's get to it! Create an error array in your code before the loop starts and populate the array at every iteration: 1 2 3 4 5 error_array = [] for f in stream : [ ... ] error = CAMERA_CENTER - line_pos error_array . append ( error ) Then, run a lap on the track and plot your error with the command plt.plot(error_array) . You should be able to map different peaks and valleys on your error plot to straight lines, 90 o and 180 o turns on the track. After this, you can create thresholds to identify when the car is in each of these scenarios. Tune your controller gains for each operating condition. While it's complicated to tune the controller individually for each operating condition, we can use our knowledge of the problem to estimate how the gains should change. Intuitively, an error on a straight line doesn't need to be corrected as aggressively as errors on turns. Therefore, let's define: 1 2 3 K_0 = 5000 K_90 = 7000 K_180 = 9000 Determine how to vary the controller gains based on the operating conditions We will use a simple discrete switch. Based on your observation of the errors, create thresholds and use them to choose your gain: 1 2 3 4 5 6 7 8 9 10 11 12 e_90 = 50 e_180 = 75 [ ... ] for f in stream : [ ... ] gain = K_0 if error > e_180 : gain = K_180 elif error > e_90 : gain = K_90 DUTY_CYCLE = SERVO_MIDDLE + gain * error Assess system performance. Use the error plot to check your car performance on the track. Is it overshooting after turns? Does it take too long to realize it's in a turn? Or maybe it takes too long to realize it's back on a straight line? You should adjust the gains and thresholds to optimize your controller and start doing some aggressive turns! Exercises You probably noticed our example only covers the proportional gain. Include integral and derivative gains in your gain scheduling routine. Test the same approach for speed scheduling. Define three different speeds (one for each operating condition) and switch among them to see if you can get faster laps. This is just one of numerous ways to apply gain scheduling to your vehicle controller. You are encouraged to try other approaches!","title":"2. Gain Scheduling"},{"location":"workshops/2_gain_scheduling/#gain-scheduling","text":"You assembled your car, connected it to your computer, tested the electronics, and hopefully got to run a few laps. So, what's next? Yoda has the answer for you! We have already encouraged you to improve the original self-driving code by adding integral and derivative terms to the PID controller. We will now introduce a new control technique called Gain Scheduling , which can be seen as an extension of PID. Note The following is a very simplified view of the topics. This is just to provide you an intuition of how the controllers work.","title":"Gain Scheduling"},{"location":"workshops/2_gain_scheduling/#feedback-control","text":"Let's start with a quick recap on feedback control. One of the simplest ways to represent a typical control problem is through a block diagram. Our goal is to make the system output y as close as possible to the reference r . In order to do that, we use a sensor to measure variable y , compute the difference between current and desired value to obtain the error e and feed it to a controller. And here is where the magic happens. The controller block is responsible for transforming the measured error e into a command u which brings the system's output y closer to the reference r . Exercise : Try to identify each of the elements in the block diagram in the context of Race On. What is the system and what is the sensor? Which variables in your code are r , e , y and u ? Controllers can take very different forms. One example is the on-off mechanism of the heaters with a thermostat: the heater turns on when the temperature is below the reference and it turns off when the temperature is above the reference. Another example is the proportional controller, which can be described by u = Ke . We can also have a controller which computes system inputs based not only on the current error, but also based on the error integral and derivative. This leads to the famous PID controller . Note, the above diagram is for continuous time systems, our car is a discrete time system since we send one control command per image and not continuously while waiting for the next image. In discrete time systems the integral is replaced with the sum operator and the derivative with the difference between current and past error. The controller gains Kp , Ki , and Kd have to be selected such that the system presents the desired behavior (fast convergence to r , low overshooting etc). If we have a mathematical model of the system, there are analytical and graphical tools that help tuning the gains. In other cases, the gains are set using trial and error. Exercises : If you haven't yet, try to implement a PID controller for your car. You can reference this pseudocode . Search online for hints on how to tune the different gains. What are the expected effects of increasing/decreasing each one of them?","title":"Feedback Control"},{"location":"workshops/2_gain_scheduling/#step-response","text":"Before we dive deeper into more advanced controllers, we first need to know how to measure the performance of different controllers. The most simple and at the same time informative method is the step response shown in the figure below. The step response is obtained by suddenly cheanging the reference from one value to another, in the figure above is from zero to one, and tracking how the controller responds to this change. To get a step response using you car, run the car on a straight line and after one second switch the reference value from CAMERA_CENTER to CAMERA_CENTER - 100 and save the values of line_pos into an array, stop the car after a timeout and plot the array to see the step response. Repeat with a step in a different direction CAMERA_CENTER + 100 to make sure your car responds to left and right turns equally. The most important metrics of the step response are rise time , overshoot , and settling time .We want a fast rise time with no overshoot and low settling time, however, in practice we have to accept a tradeoff as optimizing all three at the same time is most of the time impossible. Rise time is defined as the time from when the reference value changed until we reach the new reference (or 0.9 of the new refence value). A high gain (high value of the proportional and derivative coefficients) leads to a fast response time but also to an overshoot. Overshoot defines by how much we miss the new reference once we reach the new reference value for the first time. A high overshoot can make you car run off the track as you exit the turn, but a small overshoot can be tolerated and help decreases the rise time. Settling time is the time between the reference change and the time the system stabilizes around it. A low settling time helps if there are a sequence of turns as the car will stay closer to the reference.","title":"Step Response"},{"location":"workshops/2_gain_scheduling/#gain-scheduling_1","text":"A well tuned PID controller can yield very good performance. However, what happens if the operating condition changes? Airplanes are a good example. The system behaves remarkably different at take-off, in-flight and on landing. In these cases, the gains tuned for one operating condition might not lead to the desired behavior when the system is at another operating point. One possible solution to this problem is gain scheduling. It is usually done in 4 steps: Define the different system operating conditions; Tune your controller gains for each operating condition. This will create an array (or a table) of controller gains; Determine how to vary the controller gains based on the operating conditions; Assess system performance. Steps 1, 2, and 4 are the same as you would do for a regular PID controller. Step 3 is the fun part! The main options are: Simple switch : change the gains when the system is at a different operating point. The advantage of this method is its simplicity. However, this creates discontinuities. If the error is 2 and the gain changes from 2 to 5, your control input jumps from 4 to 10! This can damage actuators or create undesirable system behavior. Transient switch : when the operation conditions change, linearly change from one gain to the other within a certain time interval. The trick here is to choose the time interval long enough to create a smooth transient but short enough to allow the controller to respond properly to the operating condition change. Interpolate between gains : we can avoid switching by creating a curve that smoothly changes between gains continuously. This is possible when the operating conditions also change continuously. The three cases are illustrated below, where tc is the time when the operating condition change is identified, and T is the transient time for the transient switch approach. For a more in depth introduction to gain scheduling, we recommend watching this video .","title":"Gain Scheduling"},{"location":"workshops/2_gain_scheduling/#applied-to-race-on","text":"For now, let's assume your car is set to a constant speed and we only want to control the position relative to the center of the track. We will reference the original code provided with the car but it should be easy to adapt these instructions to your modified code. If you have any issues with it, remember you can always check the original code at the race on github. We follow the 4 steps described previously: Define the different system operating conditions. Even though the car itself doesn't change much during the race, the track varies considerably. We can define three different operating modes for the car: straight line, 90 o turn and 180 o turn. Next, we have to find a way to identify these conditions. One way of doing this is by looking at the error. This requires a few steps and some tuning. Let's get to it! Create an error array in your code before the loop starts and populate the array at every iteration: 1 2 3 4 5 error_array = [] for f in stream : [ ... ] error = CAMERA_CENTER - line_pos error_array . append ( error ) Then, run a lap on the track and plot your error with the command plt.plot(error_array) . You should be able to map different peaks and valleys on your error plot to straight lines, 90 o and 180 o turns on the track. After this, you can create thresholds to identify when the car is in each of these scenarios. Tune your controller gains for each operating condition. While it's complicated to tune the controller individually for each operating condition, we can use our knowledge of the problem to estimate how the gains should change. Intuitively, an error on a straight line doesn't need to be corrected as aggressively as errors on turns. Therefore, let's define: 1 2 3 K_0 = 5000 K_90 = 7000 K_180 = 9000 Determine how to vary the controller gains based on the operating conditions We will use a simple discrete switch. Based on your observation of the errors, create thresholds and use them to choose your gain: 1 2 3 4 5 6 7 8 9 10 11 12 e_90 = 50 e_180 = 75 [ ... ] for f in stream : [ ... ] gain = K_0 if error > e_180 : gain = K_180 elif error > e_90 : gain = K_90 DUTY_CYCLE = SERVO_MIDDLE + gain * error Assess system performance. Use the error plot to check your car performance on the track. Is it overshooting after turns? Does it take too long to realize it's in a turn? Or maybe it takes too long to realize it's back on a straight line? You should adjust the gains and thresholds to optimize your controller and start doing some aggressive turns! Exercises You probably noticed our example only covers the proportional gain. Include integral and derivative gains in your gain scheduling routine. Test the same approach for speed scheduling. Define three different speeds (one for each operating condition) and switch among them to see if you can get faster laps. This is just one of numerous ways to apply gain scheduling to your vehicle controller. You are encouraged to try other approaches!","title":"Applied to Race On"},{"location":"workshops/3_wheel_encoders/","text":"Wheel encoders \u00b6 In this workshop, we will introduce an additional sensor, the wheel encoders, to help estimate your car's speed. This speed information can be utilized to improve your car's racing performance. Parts and Components provided: 2x - Line sensors (wheel encoders) 2x - wheel spokes cut-outs 1x - left sensor mount 1x - right sensor mounts 2x - round cap screws 2x - nuts Installation \u00b6 Disassemble both of your rear tires. This is simply done by removing the hex screw as indicated by the red arrow in the image below. [Note: After the removal of the tires, be sure to secure the hex screw back to the axle to avoid accidentally losing any tire components, e.g. the securing pin.] You will be provided with a piece of paper containing two printed copies of the \"wheel spokes.\" Cut the outlines (both the inside and outside circles) carefully. After you finish cutting the outlines, call any of the organizer's attention for them to help you glue these wheel spokes onto your tires. See illustration below. [Note: We don't allow students to glue these by themselves, so as to prevent them from unintentionally glueing other tire components.] You are provided with line sensors (wheel encoders). Place the line sensor board onto the mount. Ensure that the securing holes of both the board and the mount are aligned properly. Insert the screw through the securing holes and place the nut on the other end of the screw to properly secure the sensor. See illustration below. Repeat this step for the second rear wheel encoder. Now it is time to install the assembled wheel encoders onto the car. First dislocate the balljoint from the wheel axle by removing the screw as indicated by the red arrow. Then remove the washer as indicated by the yellow circle (depending on the car model this washer is either black or red). You will not be needing this washer anymore. Before placing the sensors on both the rear wheels, note that the mounts provided are dedicated left and right mounts. Place the assembled wheel encoder from where you remove the washer. The sensor will have to be facing towards the center of the car rather than facing the rear. If it is otherwise, just use the other sensor mount you assembled. Ensure that the protruding secure pin from the mount is attached properly on the axle with the securing holes aligned properly. Screw back the screw to securely fix the sensor on the wheel axle. See illustration below for the final setup. Note Feel free to change the pattern on your wheels, i.e., add more black and white stripes or create a bigger gap to detect a full rotation. You can access on Overleaf the LaTeX source of the provided pattern and you can use the Overleaf editor to generate a new ones. Testing \u00b6 Before running this code we need to install an additional Python library to read the inputs from the encoder. For that run in the terminal 1 sudo apt install python3-rpi.gpio Next update the contents of your raceon.py file to match the one on the GitHub which now includes the Encoders class. Now we can test the encoders. Our test procedure will gradually increase the motor power, wait for 300 milliseconds for the motor to reach a stable speed at the new power, and then measure the wheels speed in a time window of 50 ms. The Jupyter Notebook with the testing procedure can be downloaded from GitHub . To run it, you have to save the notebook in the same folder as the raceon.py file. Note, the 4 th cell which contains the for loop will spin the wheel up to 70% power, and threfore, before running that cell pick up the car so that the wheels do not touch the floor. Usage \u00b6 Encoders give you a pulse per each transition from black to white or viceversa. Since we attached the encoders to the rear wheels which are connected throught a differential we can estimate besides the speed also the turning angle since the inside wheel will travel less distance. In what follows we will show how to measure the speed and the turning angle. Initialization \u00b6 To use the encoders we need to instantiate the Encoders class which we import from the raceon library. The class constructor requires two arguments, the signal of the left and the right encoder. For the white boards these should be 25 and 27. Place the next block of code before the motor and servo itialization code in you notebook. You need to run this code only once. 1 2 3 4 K = 0.21 / 20 # distance traveled in meters per encoder pulse Rw = 0.155 # distance in meters between the wheels centers encoders = Encoders ( 25 , 27 ) Reading data \u00b6 Once the class is instantiated, it will continuously read pulses from the encoders. The class method Nl, Nr = encoders.read() returns the number of pulses for both wheels from the last reading. Modify your Jupyter notebook code such that the for loop looks similar to this: 1 2 3 4 5 6 7 8 9 10 11 12 # Reset encoders counter encoders . read () encoders_array = [] for f in stream : [ ... ] # Read encoders value encoders_array . append ( encoders . read ()) # Get the intensity component of the image (a trick to get black and white images) This code saves the number of pulses from the encoders into the encoders_array which we will use later for track reconstruction. Estimating the speed \u00b6 To calculate the distance traveled by the car in meters we take the average of the two sensors and multiply by a conversion constant defined in the initialization part. 1 2 Nl , Nr = encoders . read () distance = ( Nl + Nr ) * K / 2 To obtaine the speed, divide the distance by the time between the two measurements, i.e., between the call of the read method. Note, having a speed estimation you can dynamically adjust the motor duty cycle so you reach your desired speed. You can do this by adding a second PID controller the actively adjusts motor duty cycle to ensure the desired speed irespective of the battery voltage or friction. Estimating turning angle \u00b6 Using differential measurements we can estimate how much the car turned since the last measurement. 1 2 Nl , Nr = encoders . read () delta_theta = ( Nl - Nr ) * K / Rw Keeping track of the sum of these angles once the turn started would give a good extimate of how much we turned. You can decrease your lap time if you can start accelerating on the straight lines before the turn ends. Recostructing the car trajectory \u00b6 Knowing the distance and the angle we can estimate the car position at each moment in time. Here we will use the previous saved array with the encoders data to plot the car trajectory. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 points = [( 0 , 0 )] theta = 0 for p in encoders_array : d = K * ( p [ 1 ] + p [ 1 ]) / 2 # distance traveled theta += K * ( p [ 1 ] - p [ 0 ]) / Rw # angle turned dp = ( points [ - 1 ][ 0 ] + d * cos ( theta ), points [ - 1 ][ 1 ] + d * sin ( theta )) points . append ( dp ) points = np . array ( points ) plt . plot ( points [:, 0 ], points [:, 1 ], '.' )","title":"3. Wheel encoders"},{"location":"workshops/3_wheel_encoders/#wheel-encoders","text":"In this workshop, we will introduce an additional sensor, the wheel encoders, to help estimate your car's speed. This speed information can be utilized to improve your car's racing performance. Parts and Components provided: 2x - Line sensors (wheel encoders) 2x - wheel spokes cut-outs 1x - left sensor mount 1x - right sensor mounts 2x - round cap screws 2x - nuts","title":"Wheel encoders"},{"location":"workshops/3_wheel_encoders/#installation","text":"Disassemble both of your rear tires. This is simply done by removing the hex screw as indicated by the red arrow in the image below. [Note: After the removal of the tires, be sure to secure the hex screw back to the axle to avoid accidentally losing any tire components, e.g. the securing pin.] You will be provided with a piece of paper containing two printed copies of the \"wheel spokes.\" Cut the outlines (both the inside and outside circles) carefully. After you finish cutting the outlines, call any of the organizer's attention for them to help you glue these wheel spokes onto your tires. See illustration below. [Note: We don't allow students to glue these by themselves, so as to prevent them from unintentionally glueing other tire components.] You are provided with line sensors (wheel encoders). Place the line sensor board onto the mount. Ensure that the securing holes of both the board and the mount are aligned properly. Insert the screw through the securing holes and place the nut on the other end of the screw to properly secure the sensor. See illustration below. Repeat this step for the second rear wheel encoder. Now it is time to install the assembled wheel encoders onto the car. First dislocate the balljoint from the wheel axle by removing the screw as indicated by the red arrow. Then remove the washer as indicated by the yellow circle (depending on the car model this washer is either black or red). You will not be needing this washer anymore. Before placing the sensors on both the rear wheels, note that the mounts provided are dedicated left and right mounts. Place the assembled wheel encoder from where you remove the washer. The sensor will have to be facing towards the center of the car rather than facing the rear. If it is otherwise, just use the other sensor mount you assembled. Ensure that the protruding secure pin from the mount is attached properly on the axle with the securing holes aligned properly. Screw back the screw to securely fix the sensor on the wheel axle. See illustration below for the final setup. Note Feel free to change the pattern on your wheels, i.e., add more black and white stripes or create a bigger gap to detect a full rotation. You can access on Overleaf the LaTeX source of the provided pattern and you can use the Overleaf editor to generate a new ones.","title":"Installation"},{"location":"workshops/3_wheel_encoders/#testing","text":"Before running this code we need to install an additional Python library to read the inputs from the encoder. For that run in the terminal 1 sudo apt install python3-rpi.gpio Next update the contents of your raceon.py file to match the one on the GitHub which now includes the Encoders class. Now we can test the encoders. Our test procedure will gradually increase the motor power, wait for 300 milliseconds for the motor to reach a stable speed at the new power, and then measure the wheels speed in a time window of 50 ms. The Jupyter Notebook with the testing procedure can be downloaded from GitHub . To run it, you have to save the notebook in the same folder as the raceon.py file. Note, the 4 th cell which contains the for loop will spin the wheel up to 70% power, and threfore, before running that cell pick up the car so that the wheels do not touch the floor.","title":"Testing"},{"location":"workshops/3_wheel_encoders/#usage","text":"Encoders give you a pulse per each transition from black to white or viceversa. Since we attached the encoders to the rear wheels which are connected throught a differential we can estimate besides the speed also the turning angle since the inside wheel will travel less distance. In what follows we will show how to measure the speed and the turning angle.","title":"Usage"},{"location":"workshops/3_wheel_encoders/#initialization","text":"To use the encoders we need to instantiate the Encoders class which we import from the raceon library. The class constructor requires two arguments, the signal of the left and the right encoder. For the white boards these should be 25 and 27. Place the next block of code before the motor and servo itialization code in you notebook. You need to run this code only once. 1 2 3 4 K = 0.21 / 20 # distance traveled in meters per encoder pulse Rw = 0.155 # distance in meters between the wheels centers encoders = Encoders ( 25 , 27 )","title":"Initialization"},{"location":"workshops/3_wheel_encoders/#reading-data","text":"Once the class is instantiated, it will continuously read pulses from the encoders. The class method Nl, Nr = encoders.read() returns the number of pulses for both wheels from the last reading. Modify your Jupyter notebook code such that the for loop looks similar to this: 1 2 3 4 5 6 7 8 9 10 11 12 # Reset encoders counter encoders . read () encoders_array = [] for f in stream : [ ... ] # Read encoders value encoders_array . append ( encoders . read ()) # Get the intensity component of the image (a trick to get black and white images) This code saves the number of pulses from the encoders into the encoders_array which we will use later for track reconstruction.","title":"Reading data"},{"location":"workshops/3_wheel_encoders/#estimating-the-speed","text":"To calculate the distance traveled by the car in meters we take the average of the two sensors and multiply by a conversion constant defined in the initialization part. 1 2 Nl , Nr = encoders . read () distance = ( Nl + Nr ) * K / 2 To obtaine the speed, divide the distance by the time between the two measurements, i.e., between the call of the read method. Note, having a speed estimation you can dynamically adjust the motor duty cycle so you reach your desired speed. You can do this by adding a second PID controller the actively adjusts motor duty cycle to ensure the desired speed irespective of the battery voltage or friction.","title":"Estimating the speed"},{"location":"workshops/3_wheel_encoders/#estimating-turning-angle","text":"Using differential measurements we can estimate how much the car turned since the last measurement. 1 2 Nl , Nr = encoders . read () delta_theta = ( Nl - Nr ) * K / Rw Keeping track of the sum of these angles once the turn started would give a good extimate of how much we turned. You can decrease your lap time if you can start accelerating on the straight lines before the turn ends.","title":"Estimating turning angle"},{"location":"workshops/3_wheel_encoders/#recostructing-the-car-trajectory","text":"Knowing the distance and the angle we can estimate the car position at each moment in time. Here we will use the previous saved array with the encoders data to plot the car trajectory. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 points = [( 0 , 0 )] theta = 0 for p in encoders_array : d = K * ( p [ 1 ] + p [ 1 ]) / 2 # distance traveled theta += K * ( p [ 1 ] - p [ 0 ]) / Rw # angle turned dp = ( points [ - 1 ][ 0 ] + d * cos ( theta ), points [ - 1 ][ 1 ] + d * sin ( theta )) points . append ( dp ) points = np . array ( points ) plt . plot ( points [:, 0 ], points [:, 1 ], '.' )","title":"Recostructing the car trajectory"}]}